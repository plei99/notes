\documentclass[10pt, twoside]{article}
\usepackage{../../../notes}
\usepackage{enumitem}
%\usepackage{mathabx}
\setlist{noitemsep}
%\geometry{letterpaper, margin=2cm}
\newcommand{\borel}{\mathcal{B}}
\renewcommand{\d}{\ \mathrm{d}}


\title{Math 624 Lecture Notes}
\author{\scshape Taught by Robin Young, Notes by Patrick Lei}
\affil{\itshape University of Massachusetts, Amherst}
\date{Spring 2019}

\fancypagestyle{firstpage}
{
   \fancyhf{}
   \fancyfoot[R]{\itshape Page \thepage\ of \pageref{LastPage}}
   \renewcommand{\headrulewidth}{0pt}
}


\fancypagestyle{pages}
{
    \fancyhf{}
    \fancyhead[LO]{\scshape Math 624}
    \fancyhead[RO]{\scshape Lecture Notes}
    \fancyhead[CO]{\scshape Analysis 2}
    \fancyhead[CE]{\scshape University of Massachusetts, Amherst}
    \fancyhead[LE]{\scshape Patrick Lei}
    \fancyhead[RE]{\scshape Spring 2019}
    \fancyfoot[RO,LE]{\itshape Page \thepage \space of \pageref{LastPage}}
    \renewcommand{\headrulewidth}{0.1pt}
}

\pagestyle{pages}

\begin{document}

    \maketitle\thispagestyle{firstpage}
    
    \begin{abstract}
        Continuation of Math 623. Introduction to functional analysis; elementary theory of Hilbert and Banach spaces; functional analytic properties of Lp-spaces, applications to Fourier series and integrals; interplay between topology, and measure, Stone-Weierstrass theorem, Riesz representation theorem. Further topics depending on instructor.
    \end{abstract}

    \tableofcontents

    \section{Organizational}
    A webpage (\url{http://people.math.umass.edu/~young/analysis/index.html}) has been put up with one topic per week, which is impractical. We will aim to get through ten topics. Formal office hours will not be scheduled because this is the only class Robin is teaching. The understanding is that we will visit his office. At some point during lecture, the electrical engineering student walked into the room anad had the following conversation with Robin:

    \begin{description}
        \item[Robin] Are you Bo?
        \item[Bo] Yes
        \item[Robin] Everyone, this is Bo. Bo, this is everyone.
        \item[Bo] I am in the electrical engineering department. This will help me with my research.   
    \end{description}

    \section{Review of Last Semester}

    \subsection{Lecture 1 (Jan 22)}
    We will review measure theory from an abstract perspective and treat the Lebesgue measure as an example.\footnote{It happens to the unique Haar measure on $\R^n$} Let $X$ be a locally compact topological space. 
    
    \begin{defn}[Sigma-Algebra]
        A $\sigma$-algebra $\Sigma$ to be a collection of subsets closed under complement and countable union.
    \end{defn}

     We denote by $\mathcal{B}$ the Borel $\sigma$-algebra, which is the smallest such containing all open sets. $\mu$ is a (non-negative) measure if $\mu$ is defined on $\Sigma$, the empty set has measure $0$, and $\mu$ is countably additive. 

    \begin{rmk}[Notation]
        Indexing with a Roman letter means countable and indexing with a Greek letter means uncountable.
    \end{rmk}

    \begin{defn}[Sigma-finite]
        A measure $\mu$ is $\sigma$-finite if $\mu(X) = \sum_{i=1}^{\infty} \mu(E_i)$ where each $E_i$ has finite measure.
    \end{defn}

    \begin{defn}[Completeness]
        A measure space $(X,\Sigma, \mu)$ is complete if all subsets of measure zero sets are measurable: $\mu(B) = 0$ and $A \subset B$ implies $A \in \Sigma$.
    \end{defn}

    \begin{defn}[Measurable function]
        A function $f: X \rightarrow R$ is measurable if $f^{-1}(\alpha, \infty)$ is measurable for all real $\alpha$.
    \end{defn}

    Given the notion of a measurable function, we can integrate:

    \begin{defn}[Simple Function]
        A simple function is a finite linear combination of indicator functions \[ \varphi = \sum_{i=1}^k a_i\chi_{E_i}. \]
    \end{defn}

    We define the integral of a simple function $\varphi = \sum_{i=1}^k a_i \chi_{E_i}$ to be \[\int \varphi\ \mathrm{d}\mu = \sum_{i=1}^k a_i \mu(E_i).  \]

    Given a nonnegative measurable function $f$, we define the integral \[\int f\ \mathrm{d}\mu = \sup\left\{\int \varphi\ \mathrm{d} \mu\right\},\] where $\varphi$ ranges over all nonnegative simple functions less than or equal to $f$ almost everywhere.

    We also set, for a measurable set $E$, \[\int_{E} f\ \mathrm{d}\mu = \int f\chi_{E}\ \mathrm{d}\mu. \]

    \begin{exm}
        Given the trivial sigma-algebra, only constant functions are simple.
    \end{exm}

    \begin{lem}
        Any measurable $f$ can be approximated from below by $\phi_k \rightarrow f$ with $\int \varphi_k\ \mathrm{d}\mu \rightarrow \infty$.
    \end{lem}

    We observe that Fatou's Lemma, monotone convergence, and dominated convergence hold.

    \begin{lem}[Convergence theorems]
        If $\mu_n \rightarrow \mu$ setwise on $\Sigma$ and if $f_n \rightarrow f$ almost everywhere (with respect to $\mu$) on $X$, then \[\int f_n \d \mu \rightarrow \int f \d \mu.\]
    \end{lem}

    The ability to exchange limits and integrals is the motivation behind measure theory, and more broadly, analysis is about exchanging limits.

    \begin{defn}[Mutually Singular]
        Two measures $\mu, \nu$ are mutually singular, written $\mu \perp \nu$ if there exist disjoint $A,B$ such that $X = A \bigcup B$ such that $\mu(B) = \nu(A) = 0$.
    \end{defn}

    \begin{exm}[Cantor]
        Consider the Cantor set $C$ and Cantor function $f_C$. Recall that the Cantor function is a continuous surjection from $[0,1]$ to itself and has derivative zero almost everywhere. In particular, $f_C$ violates the fundamental theorem of calculus. Define the Cantor measure on the Borel sets in $[0,1]$ the usual way by $\mu_C((a,b]) = f_C(b) - f_C(a)$. We see that $\mu_C(C) = 1$, but $\lambda(C) = C$ where $\lambda$ is the Lebesgue measure. Therefore, $\mu_C \perp \lambda$ on $[0,1]$.
    \end{exm}

    \begin{defn}[Absolute Continuity]
        Given two measure $\mu, \nu$, $\nu$ is absolutely continuous ($\nu \ll \mu$) with respect to $\mu$ if $\mu(A) = 0$ implies that $\nu(A) = 0$.
    \end{defn}

    \begin{exm}
        Given any nonnegative integrable $f$ with respect to $\mu$, we can define a measure $\mu_f(E) = \int_E f \d \mu$. Then $\mu_f \ll \mu$.
    \end{exm}

    \begin{thm}[Radon-Nikodym]
        If $\nu \ll \mu$ on a $\sigma$-finite $X$ (with respect to $\mu$), then there exists a non-negative measurable $f$ such that $\nu = \mu_f$. We write $f = \frac{d\nu}{d\mu}$.
    \end{thm}

    \begin{exm}
        Let $f$ be monotone increasing. Recall that left and right limits exist everywhere. Then observe that $f$ has at most countably many discontinuities $x_j$ and \[ \sum_{j=1}^{\infty} f(x_j+) - f(x_j-) < \infty \] for $x_j$ restricted to a compact set. We can write $f = f_j + f_c$ where $f_j$ encodes the jumps and $f_c$ is continuous. Then $f_c$ decomposes into $f_c = f_a + f_C$, where $f_a$ is absolutely continuous with respect to the Lebesgue measure, and $f_C$ is the ``Cantor'' part. To extract $f_a$ we will consider the points at which $f$ is differentiable. In the end, we will have $f = f_a + f_C + f_j$, where $f_a$ is absolutely continuous, $f_C$ is ``Cantor,'' and $f_j$ denotes the jumps. The three induced measures will be mutually singular.
    \end{exm}

    \subsection{Lecture 2 (Jan 24)}%
    \subsubsection{Inanities}%
    Robin came in late and we had a conversation about Hagoromo Fulltouch chalk and chalk holders. Apparently those are like mechanical pencils. Robin would like to continue with the review and do a calculation.

   \subsubsection{Review Continued}%
   Recall that we have a monotone increasing function $F$, which we will assume is right-continuous. Recall that we can decompose $F = F_j + F_a + F_C$ as we did yesterday.
   There are several questions: 
   \begin{enumerate}
       \item How do we find this decomposition?
       \item What do we do with it?
   \end{enumerate}

   To find $F_j$, the jump part, we can locate jumps as the discontinuities. Let $J$ be the set of jumps. Then \[ F_j = \sum_{x_i \in J} (F(x_i+) - F(x_i-))\cdot H(x-x_i), \] where $H$ is the Heaviside function. Then we write the measure \[dF_j = \sum_i (F(x_i+) - F(x_i-)) \cdot \delta_{x_i},\] where $\delta$ is the Dirac mass.

    Now to find $F_a$, we differentiate $F$. Recall that we can compute the Dini derivatives everywhere. We also proved in the fall that the set of points of nondifferentiability has measure zero with respect to Lebesgue. Then, we can integrate $F'$ with respect to $\lambda$ to get \[F_a = \int F' \d x.\]

    We know that $F(b) - F(a) \geq \int_a^b F'(x) dx$, so $F_a$ is the absolutely continuous part of $F$. Finally, the ``Cantor'' part $F_C$ is the remainder $F_C = F-F_j-F_a$.
    
    The bottom line is that it is hard to locate the ``Cantor'' part of a function.
    
    \begin{exm}
        Suppose $F(x) = \begin{cases}
            x & x < 0 \\
            1 & 0 \leq x < 2 \\
            x^2 & x \geq 2
        \end{cases}$ with jumpts at $0,2$. Then we see that $F_j(x) = H_0(x) + 3H_2(x)$ We can differentiate to find $F'(x) = \begin{cases}
            1 & x < 0 \\
            0 & 0 \leq x < 2 \\
            2x & x \geq 2
        \end{cases}$. Then, integrating, we find that $F$ has no ``Cantor'' part. Now we see that the Lebesgue-Stieltjes measure is $\d \mu = F' \d \lambda + \delta_0(x) + 3 \delta_2(x)$ and integrating some function $g(x)$ is something like \[\int_{[-4,7]}g(x) \d\mu = \int_{-4}^0 g(x) \d x + g(0) + \int_0^2 0 + 3g(2) + \int_2^7 g(x)\cdot 2x \d x.\]
    \end{exm}
    
    Conversely, given a measure $\mu$, we can define the corresponding function $F$ satisfying $\mu(a,b] = F(b) - F(a)$. Define $F$ by $F(y) = F(0) + \mu(0,y]$ for positive $y$ and similarly for negative $y$

    Recall again the Radon-Nikodym Theorem:
    \begin{thm}[Radon-Nikodym]
        Let $(X, \Sigma, \mu)$ be a $\sigma$-finite measure space and $\nu$ another $\sigma$-finite signed measure on $\Sigma$. Then there exist unique measures $\nu_s, \nu_a$, where $\nu_s \perp \mu, \nu_a \ll \mu$, such that $\nu = \nu_s + \nu_a$. Moreover, there exists $f_a$ integrable such that $\nu_a(E) = \int_E f_a \d\mu$ for all measurable $E$. We write $f_a = \frac{\d\nu}{\d\mu}$ the Radon-Nikodym derivative.
    \end{thm}

    Now recall signed measures:
    \begin{defn}[Signed Measure]
        A signed measure is a countably additive function on $\Sigma$ that vanishes on the empty set and does not take on both positive and negative infinity.
    \end{defn}

    \begin{lem}
        There exist $A,B \subset X$ such that $\nu(E) \geq 0$ for all $E \subset A$ and $\nu(E) \leq 0$ for all $E \subset B$.
    \end{lem}

    We can set $\nu_+(E) = \nu(E \bigcap A)$ and $\nu_-(E) = -\nu(E \bigcap B)$. Then we have $\nu(E) = \nu_+(E) - \nu_-(E)$ and $\abs{\nu(E)} = \nu_+(E) - \nu_-(E)$. In addition, the decomposition $\nu = \nu_+ - \nu_-$ is unique. Similarly, we can define complex measures. Then $\abs{\nu}$ is the ``total variation'' of $\nu$.

    \section{Product Measures and Fubini}%
    
    \subsection{Lecture 2 continued}%
    Our first issue: are products of measurable sets measurable? What is the correct product $\sigma$-algebra? We will simplify the structure.

    \begin{defn}[Semi-algebra]
        A semi-algebra $\mathcal{C}$ satisfies: if $A,B \in \mathcal{C}$, then $A \bigcap B \in \mathcal{C}$ and $A^C$ can be written as a finite disjoint union $A^c = \sqcup_{i=1}^n B_i$, where $B_i \in \mathcal{C}$.
    \end{defn}
    
    This is very convenient for Cartesian products because $( A \times B ) \bigcap (C \times D) = (A \bigcap C) \times (B \bigcap D) $ and $(A \times B)^C = (A \times B^C) \sqcup (A^C \times B) \sqcup (A^C \times B^C)$.

    Generate an algebra $\mathcal{A}$ from our semi-algebra $\mathcal{C}$ by $\emptyset$ plus finite unions of elements of $A$. Now suppose we have an additive set function $\mu$ defined on $\mathcal{C}$, this extends to a measure on $A$ provided that:

    \begin{enumerate}
        \item If $C \in \mathcal{C}$ is a finite union of disjoint union of elements of $\mathcal{C}$ $C = \sqcup_{i=1}^k B_i$, then $\mu(C) = \sum_{i=1}^k \mu(B_i)$.
        \item $\mu$ is countably subadditive on $\mathcal{C}$.
    \end{enumerate}

    Given $(X, \Sigma_x, \mu)$ and $(Y,\Sigma_y,\nu)$, we develop the product measure $\mu \times \nu$ on $X \times Y$. First let $\mathcal{C} = \Sigma_x \times \Sigma_y$. Obviously, $\mathcal{C}$ is a semialgebra, so it extends to an algebra $\mathcal{A}$. Now define $\mu \times \nu$ on $\mathcal{C}$ by $\mu \times \nu (A \times B) = \mu(A)\nu(B)$.

    \begin{lem}
        $\mu \times \nu$ satisfies the above two conditions.
    \end{lem}

    \subsection{Lecture 3 (1/29)}

    \subsubsection{An Apology}
    Robin would like to offer an apology. He realizes that this is the first time he's teaching the class, so we are all guinea pige here. He has come to realize that Stein's book is not very good. It's boring, encyclopedic, and not very well arranged. It has too much measure theory, and he apologizes to us for spending too much time in the fall on measure and integration, which is really boring. After we summarize Fubini in a general setting, we will take measure theory for granted and move on to something more interesting.

    Robin says that the cardinal rule of teaching is to never end on time.

    \subsubsection{Product Measures/Fubini Continued}
    Recall the definition of a semi-algebra from last time.

    We restate the lemma from last time: 

    \begin{lem}
        Suppose we have an additive set function $\mu$ defined on $\mathcal{C}$, this extends uniquely to a measure on $A$ provided that:

    \begin{enumerate}
        \item If $C \in \mathcal{C}$ is a finite union of disjoint union of elements of $\mathcal{C}$ $C = \sqcup_{i=1}^k B_i$, then $\mu(C) = \sum_{i=1}^k \mu(B_i)$.
        \item $\mu$ is countably subadditive on $\mathcal{C}$.
    \end{enumerate}
    \begin{proof}
        If $C = \sqcup C_i = \sqcup D_i$, write each $C_i$ as $C_i = \sqcup (C_i \bigcap D_j)$, so $\mu(C_i) = \sum \mu(C_i \bigcap D_j)$. For the second part, we show that $\mu(\sqcup C_i) = \sum \mu(C_i)$. We see that $\sqcup_{i=1}^n C_i \subset C = \sqcup C_i$ for all $n$. Then we see that \[\sum_{i=1}^n \mu(C_i) = \mu(\sqcup_{i=1}^n C_i) \leq \mu(C),\] so letting $n \rightarrow \infty$, we have our desired result.
    \end{proof}
    \end{lem}

    Now we return to our product construction from last time. 

    \begin{lem}
        $\lambda = \mu \times \nu$ satisfies the above two conditions.
        \begin{proof}
            First we show that if $\Set{A_i \times B_i}$ is a countable disjoint collection with $A \times B = \bigcup A_i \times B_i$, then $\lambda(A \times B) = \sum \lambda(A_i \times B_i)$. To see this, fix $x \in A$. For all $y \in B$, we see that $(x,y)$ is in only one $A_i \times B_i$, so $B = \sqcup_{\Set{i | x \in A_i}} B_i$, so as a function on $X$, $\nu(B)\chi_A(x) = \sum \nu(B_i) \chi_{A_i}(x)$ by countable additivity. Integrating with respect to $\mu$ and using monotone convergence, we have \[ \sum_i \int \nu(B_i)\chi_{A_i}(x)\d \mu = \int \nu(B) \chi_A(x) \d \mu, \] and therefore $\sum_i \nu(B_i)\mu(A_i) = \nu(B)\mu(A)$.
        \end{proof}
    \end{lem}

    However, to achieve our goal, we need to extend this algebra $\mathcal{R'}$ to a $\sigma$-algebra so we can take limits. In order to do this, we need to perform Caratheodory's construction and use outer measures:  \[ \lambda_*(E) = \inf \Set{\sum_{i=1}^n \lambda(R_i) | E \subset \bigcup_{i=1}^n R_i}. \]

    Then we declare $E$ to be measurable if $\lambda_*(A) = \lambda_*(A \bigcap E) + \lambda_*(A \bigcap E_C)$ for all $A$. Then conclude that thereis a complete $\sigma$-algebra. $\mathcal{S} \supset \mathcal{R'}$ on which the product measure is defined.

    Now we integrate with respect to $\mu \times \nu$. In order to prove Fubini, as before we denote the slice $E_x = \Set{y | (x,y) \in E}$.

    \begin{lem}
        $E_x$ is measurable if $E \in \mathcal{R}_{\sigma\delta}$, where $\mathcal{R} = \mathcal{R'}$, $\mathcal{R}_{\sigma} = \Set{\bigcup _{i=1}^{\infty} R_i | R_i \in \mathcal{R}}$ and $\mathcal{R}_{\sigma\delta} = \Set{\bigcap_{j=1}^{\infty} R_j | R_j \in \mathcal{R}_{\sigma}}$. In addition, $\mathcal{R}_{\sigma\delta}$ is a $\sigma$-algebra containing $\mathcal{R'}$, and $\mu \times \nu$ can be defined on the completion of $\mathcal{R}_{\sigma\delta}$.
    \end{lem}

    \begin{lem}
        If $E \in \mathcal{R}_{\sigma\delta}$ and $\mu \times \nu(E)$ is finite, then $g:X \rightarrow \R$ defined by $g(x) = \nu(E_x)$ is measurable and $\int g \d\mu = \mu\times\nu(E)$.
    \end{lem}

    \begin{lem}
        If $\mu \times\nu(E)=0$, then for almost every $x$, $\nu(E_x) = 0$.
    \end{lem}

    \begin{thm}
        Let $E$ be a measurable subset of $X \times Y$ with $\mu \times\nu(E)$ finite. Then for almost every $x$, $E_x$ is measurable and $g(x) = \nu(E_x)$ is a measurable function and $\int g(x) \d \mu = \mu\times\nu(E)$.
    \end{thm}

    \begin{thm}[Fubini]
        \begin{enumerate}
            \item $f_x:Y \rightarrow \R$ defined by $f_x(y) = f(x,y)$ is integrable on $Y$ for almost every $X$. The same statement holds if we switch the roles of $X$ and $Y$.
            \item $\int_Y f_x(y) \d\nu(y)$ is an integrable function on $X$. Again, the same statement holds for $Y$.
            \item $\int_X \left(\int_Y f_x \d\nu\right)\d\mu = \int_Y \left(\int_X f_y \d\mu\right)\d\nu = \int_{X \times Y} f \d(\mu\times\nu)$.
        \end{enumerate}
    \end{thm}

    \begin{thm}[Tonelli]
        The same conditions in Fubini's theorem hold for non-negative measurable functions on $X \times Y$.
    \end{thm}

    \begin{exm}[Counterexample if non-negativity is not required]
        Consider $f = \frac{x^2-y^2}{x^2+y^2}$ (see the homework).
    \end{exm}

    We are going to assume now that integration over a general measure has been mastered. After this, we will talk about convergence of functions. Because spaces of functions are infinite-dimensional, we don't have nice convergence results that exist in the finite-dimensional case. We will see that the most important norms are $L^1, L^2, L^{\infty}$, which correspond to integration, Euclidean, and uniform, respectively. We will treat all of our Banach spaces abstractly. ``Abstraction makes things easier generally. However, when applying this stuff, we have to remove ourselves from this abstruct into real stuff.''

    ``We stopped early and banked three minutes so I don't have to stop early again.''

    \section{Notions of Convergence}
    \subsection{Lecture 4 (Jan 31)}

    In undergrad analysis and last semester, we always think about $\R^n$. Now, we're going to always think about infinite-dimensional spaces.
    
    Recall that in $\R^d$, we can have sequences $x_n \rightarrow x$ This is really the only form of convergence in a finite dimensional space. Functions can converge in several ways:
    \begin{description}
        \item[Pointwise] This is easy but not very helpful. Functions are only defined up to sets of measure zero. This is essentially convergence in $\R$ at each $x \in X$.
        \item[Uniform] This is much better than pointwise convergence. For example, a uniform limit of uniformly continuous $f$ is uniformly continuous. This is the same thing as convergence in the $L^{\infty}$ norm $\norm{f}_{\infty} = \sup_{x \in X} \abs{f(x)}$. 
        \item[Convergence in Measure, $L^1$, etc.] These were considered last semester. 
    \end{description}

    We will consider abstract convergence in metric spaces. Let $(X,d)$ be a metric space. Then $X$ inherits a topology given by the metric. 

    \begin{lem}
        Let $(X,d)$ be a metric space and $F \subset X$. Then $F$ is closed if and only if every convergent sequence in $F$ has a limit in $F$.
    \end{lem}

    This gives us a characterization of closure using sequences.

    \begin{exm}[Continuity]
        Continuity on a metric space can be defined the same way as continuity in calculus. Alternative equivalent definitions include sequential continuity ($f$ is continuous if it commutes with limits of sequences)and topological continuity. The first one is the most tangible and connected to $\R$, the second is the easiest to use in practice (but highly dependent on the notion of completion), and the third is the most abstract. 
    \end{exm}

    \begin{defn}[Density]
        $A$ is dense in $X$ if $\overline{A} = X$.
    \end{defn}

    \begin{rmk}
        If $A$ is dense, then for all $x$ there exists a sequence in $A$ converging to $x$. Typically, if we can establish a property on $A$, we can extend to $X$ by passing to the limit.
    \end{rmk}

    \begin{exm}
        $C^{\infty}$ is dense in $L^1$. We saw this last semester when we discussed convolution. Recall that if $K_{\delta}$ is a nice kernel and $u \in K^1$, then $u * K_{\delta} \in C^{\infty}$ and the convolutions converge to $u$. Thus, if we can define an operator on smooth functions, we can pass to the limit to define the operator on a much bigger class.
    \end{exm}

    \begin{exm}[Heat equation]
        The heat equation is $\Delta_t = \Delta u$. This ``looks like'' $\frac{\d u}{\d t} = Au$ where $A = \Delta$ and $u=e^{tA}u_0$. So we'd like to write $u(t) = e^{t\Delta}u_0$ where $\Delta $ is the Laplacian. The operator $e^{t\Delta}$ doesn't make sense for general functions $u_0(x)$, but we can define it on $C^{\infty}$ using the power series. Then we can extend this to $L^1$ by taking limits.
    \end{exm}

    \subsubsection{Completeness}
    This is the same as the topological completion of $\Q$ to give $\R$. Recall that there are two constructions: Dedekind cuts, which are easier, and Cauchy sequences, which are better.\footnote{There was a question about Dedekind cuts, but please refer to Wikipedia instead.} Here, we will use Cauchy sequences which can be done in any metric space.

    \begin{defn}
        $\tilde{X}$ is a completion of $X$ if:
        \begin{enumerate}
            \item There exists an isometry $i: X \rightarrow \tilde{X}$;
            \item $i(X)$ is dense in $\tilde{X}$;
            \item $\tilde{X}$ is complete.
        \end{enumerate}
    \end{defn}

    \begin{thm}
        Any metric space $X$ has a unique completion $\tilde{X}$ up to isometry.\footnote{We are not defining isometries to be bijective}\footnote{Isometry and equivalence of metrics are not the same. Two metrics are equivalent if they generate the same topology.}
    \end{thm}

    \begin{exm}[Equivalence of metrics]
        Let $X$ be a positive definite symmetric matrix. Then $d_A(x,y) = x^TAy$ is a metric with the ``unit ball'' the ellipsoid with axes given by $\lambda_i$. Then we see that $d_A$ is equivalent to the Euclidean metric, but not isometric.
    \end{exm}

    \begin{rmk}
        All isometries are injective. An invertible isometry is an isomorphism in the category of metric spaces.
    \end{rmk}

    \begin{proof}[Proof of Theorem 38]
            Suppose we have two completions $\tilde{X}_1, \tilde{X}_2$ of $X$. Then we find an isomorphism between $\tilde{X}_1, \tilde{X}_2$. 
            
            Because $X$ is dense in $\tilde{X_1}$, we use the definition of $i_2$ on $X$ to derive $\hat{i}$. Given any $x \in \tilde{X}_1$, there exists a sequence $\Set{x_k}$ in $X$ where $x_k \rightarrow z$. Then we define $\hat{i}z = \lim i_2x_k$. 
            
            We see that clearly this is well-defined because two sequences that converge to the same point must approach each other. To check that $\hat{i}$ is an isometry, $d(z_1, z_2) = \lim d(x_k \overline{x}_k) = \lim d_2(i_2x_k, i_2\overline{x}_k) = d_2(\hat{i}z_1, \hat{i}z_2)$. Also, note that $\hat{i}i_1x = i_2x$ for all $x \in X$. Finally, note that if $z_2 \in \tilde{X}_2$, choose $x_k \in X$ such that $i_2x_k \rightarrow z_2$. Then $z=\lim i_1x_k$ satisfies $\hat{i}z = z_2$.

            Existence is seen using Cauchy sequences.
    \end{proof}

    \section{Compactness}

    \subsection{Lecture 5 (Feb 5)}

    

    The notion of compactness distinguishes the two cases of finite and infinite dimensions. There are two notions of compactness: sequential and topological. A prototypical example of a compact set in $\R$ is the closed interval $[0,1]$.

    \begin{defn}[Sequentially Compact]
        Let $X$ be a metric space. $K \subset X$ is sequentially compact if every sequence $\{x_k\} \subset K$ has a convergent subsequence with limit in $K$.
    \end{defn}

    \begin{thm}[Heine-Borel]
        $K \subset \R^d$ is sequentially compact if and only if it is closed and bounded.
    \end{thm}

    This follows easily from the Bolzano-Weierstrass theorem, which states that every bounded sequence in $\R^d$ has a convergent subsequence. Robin explained the idea of $B-W$, but I will assume that you already know it. In the context of a metric space, we introduce the notion of ``total boundedness.''

    \begin{defn}[$\ep$-net]
        Given a set $B$, an $\ep$-net $N_{\ep}$ is a subset of points of $B$ such that \[B \subset \bigcup_{x \in N_{\ep}} B_{\ep}(x).\]
    \end{defn}

    \begin{defn}[Totally Bounded]
        A set $A$ is totally bounded if and onlg if $A$ has a finite $\ep$-net for all $\ep > 0$.
    \end{defn}

    \begin{lem}
        If $X$ is totally bounded, then it is separable.
        \begin{proof}
            For all $n$, we extract a finite $1/n$-net $A_n$. Now let $A = \bigcup_{n=1}^{\infty} A_n$, which is countable. We show that $A$ is dense in $X$.

            Given $x \in X$, for any $\ep$, there exists $n > \frac{1}{\ep}$, but then there exists $y \in A_n$ such that $d(x,y) < \frac{1}{n} < \ep$.
        \end{proof}
    \end{lem}

    \begin{thm}
        $K \subset X$ is sequentially compact if and only if it is complete and totally bounded.\footnote{This is the metric space analogue of Heine-Borel.} Cauchy subsequence.
        \begin{proof}
            First assume that $K$ is complete and totally bounded. Then given any sequence $\{x_n\} \subset K$ we extract a subsequence converging in $K$. By completeness, we extract a Cauchy subsequence. 
            
            Choose $x_{n_1}$ and suppose inductively we have the subsequence $x_{n_1} \ldots, x_{n_k}, y_1^k, \ldots, y_m^k, \ldots$ chosen at the $k$-th step. We want the subsequence $\{x_{n_k}\}$ to be Cauchy. Each subsequence is totally bounded. At the $k$-th inductive step, set $\ep = \frac{1}{2^{k+1}}$. Then the set $Y_k = \{y_1^k, \ldots, y_m^k, \ldots\}$ is totally bounded, so $Y_k$ has a finite $\ep$-net. Then there exists a finite $\ep$-net $\{x_1, \ldots, z_m\}$ such that $Y_k \subset \bigcup_{i=1}^m B_{\ep}(z_j)$. Thus there exists one $B_{\ep}(z_j)$ containing infinitely many $\{y_k\}$. Choose $x_{n_{k+1}} = z_j$ and $y_1^{k+1}, \ldots, y_m^{k+1}, \ldots$ which are in $B_{\ep}(z_j)$. Clearly $x_{n_{k+1}}$ is Cauchy by construction.

            For the converse, let $\{x_k\}$ be Cauchy in $K$. Then there exists a convergent subsequence $x_{n_k} \rightarrow x \in K$. The limit of any subsequence is the limit of the full sequence because the sequence is Cauchy, so $K$ is complete.

            Now assume $K$ is not totally bounded. We find a sequence with no convergent subsequence. Then there exists $\ep > 0$ such that $K$ has no fintie $\ep$-net, i.e.for any finite $\ep$-net, there exists $x \in K$ not within $\ep$ of the net. For each $n$, having chosen $\{x_1, \ldots, x_n\}$, choose $x_{n+1}$ such that $x_{n+1} \in K$ but $x_n+1 \notin \bigcup_{j=1}^n B_{\ep}(x_j)$. By construction, for all $n > m$, $d(x_n, x_m)  >\ep$, so this sequence has no Cauchy subsequence.
        \end{proof}
    \end{thm}

    \begin{defn}[(Topological) Compactness]
        $K$ is compact if every open cover has a finite subcover.
    \end{defn}

    In a metric space, compactness and sequential compactness are equivalent. However, there are examples of compact sets not sequentially compact and sequentially compact sets not compact.

    \begin{thm}
        Let $X$ be a metric space. $K \subset X$ is compact if and only if it is sequentially compact.
        \begin{proof}
            Let $K$ be sequentially compact. Given $G_{\alpha}$ an open cover of $K$, we wish to extract a finite subcover. 
            
            First we use total boundedness to extract a countable subcover. We know $K$ is totally bounded, so it is separable. Let $A$ be a countable dense subset of $K$. Let $\mathcal{B} = \Set{B_r(a) | r \in \Q, a \in A}$, which is countable. Then let $\mathcal{C} = \Set{B_i \in \mathcal{B} | B_i \subset G_{\alpha} \text{ for some $\alpha$}}$. We use $\mathcal{C}$ to extract a countable subcover $\{G_n\}$ of $K$. Let $x \in K$. Then there exists $\alpha$ such that $x \in G_{\alpha}$. Then there exists $\ep$ such that $B_{\ep}(x) \subset G_{\alpha}$. Now there exists $y \in A$ such that $d(x,y) < \frac{ep}{3}$. Also, $x \in B_{\frac{2\ep}{2}}(y) \subset B_{\ep}(x) \subset G_{\alpha}$. Now choose $q \in \Q$ such that $\frac{ep}{3} < q < \frac{2\ep}{3}$. Then $B_{q}(y) \in \mathcal{C}$. Thus $\mathcal{C}$ is a countable open cover of $X$. For any $B \in \mathcal{C}$, there exists $\alpha_B$ such that $B \subset G_{\alpha_B}$. These $G_{\alpha_B}$ form a countable subcover of $\{G_{\alpha}\}$. We relabel them as $G_n$.

            Suppose there exists no finite subcover of $G_n$. Then for all $N$, there exists $x \in K \setminus \bigcup_{n=1}^N G_n$. Choose $x_1 \in K$. Then there exists $N_1$ such that $x_1 \in G_{N_1}$, but there exists $x_2 \in K$ such that $x_2 \notin \bigcup_{n=1}^{N_2} G_n$. There must exists $N_2$ such that $x_2 \in G_{N_2}$. Inductively, choose a sequence $\{x_k\}$ and a strictly increasing sequence $\{N_{k+1}\}$ of integers. For all $n$, there exists $M_n$ such that whenever $m > M_n$, $x_m \notin G_n$. Therefore if $x \in G_n$, there is no subsequence $x_{k_n} \rightarrow x$. But the $G_n$ cover $K$, $\{x_k\}$ has no subsequence converging to any $x \in K$, so $K$ is not sequentially compact.

            In the other direction, suppose $K$ is not sequentially compact. Then there exists a sequence $\{x_k\}$ containing no convergent subsequence. Without loss of generality, suppose the $x_k$ are distinct. Then let $x \in K$. If $B_{\ep}(x)$ cpmtaoms an $x_k \neq k$ for all $\ep$, then that subsequence converges to $x$. Thus there exists $\ep_x$ such that $B_{\ep_x}(x)$ has no element of $\{x_k\}$ or $x = x_k$ for some $k$. But then $B_{\ep_x}(x)$ is an open cover. Thus there exists a finite subcover, which implies that there are only finitely many distinct elements of the sequence, which is a contradiction.
        \end{proof}
    \end{thm}

    \subsection{Lecture 6 (Feb 7)}
    Robin actually arrived to lecture early today. This has never happened before in the history of this analysis class with the exception of the first lecture last semester.

    Today we will carry on with compact sets and prove some theorems. One remark is that if you lose a derivative, you gain compactness. 

    \begin{thm}
        Let $f : K \rightarrow Y$ is continuous with $K$ compact and $Y$ a metric space. Then $f(K)$ is also compact.
        \begin{proof}
            Let $\{G_{\alpha}\}$ be an open cover of $f(K)$. We pull back to an open cover of $K$, extract a finite subcover, and then send this open cover back to the image.
        \end{proof}
    \end{thm}

    \begin{thm}
        If $f: K \rightarrow Y$ from a compact set $K$ is continuous, then it is uniformly continuous. 
        \begin{proof}
            There exists $\ep$ such that for any $n$, we can find $x_n,y_n$ such that $d(x_n,y_n) < \frac{1}{n}$, but $d(f(x_n), f(y_n)) > \ep$. Now extract convergent subsequences $\{x_{n_k}\}$ and $\{y_{n_k}\}$. Then $x_{n_k} \rightarrow x$, $y_{n_k}$, but $d(f(x), f(y)) \geq \ep$, which contradicts continuity because $x = y$.
        \end{proof}
    \end{thm}

    \begin{thm}
        Let $f: K \rightarrow \R$ be continuous on a compact $K$. Then $f$ attains a maximum and minimum.
        \begin{proof}
            Let $\beta = \sup_{z \in K} f(z)$. Then for all $n$, there exists $x_n$ such that $\beta - \frac{1}{n} < f(x_n) \leq \beta$. Extract a convergent subsequence $x_{n_k} \rightarrow x \in K$. By continuity $f(x) = \lim_{k \rightarrow \infty} f(x_{n_k}) = \beta$.
        \end{proof}
    \end{thm}

    \begin{cor}
        If $f : K \rightarrow \R$ is upper semi-continuous (resp. lower semi-continuous), then it is bounded above (resp. below) and attains its maximum (resp. minumum).
    \end{cor}

    \begin{rmk}
        The theorem says nothing about the uniqueness of optimizing sequences. An example is $f: \R^n \rightarrow \R$ given by $f(x) = \abs{x}\left(1-\cos\left(\frac{1}{x}\right)\right)$. Then there are concentric circles of minima.
    \end{rmk}

    \subsubsection{Compactness in Infinite Dimensions}
    We introduce our metric space as $C(X)$ the set of continuous real-valued functions on compact $X$. Use the norm $\norm{f} = \sup_{x \in X}\abs{f(x)}$. Let $C_b(X)$ denote the set of bounded functions $X \rightarrow \R$. Then $C_b(x)$ is a Banach space (a complete normed vector space). In fact $C_b(K)$ is a Banach algebra: $\norm{fg} \leq \norm{f} \norm{g}$.

    \begin{thm}
        If $K$ is compact, then $C(K)$ is complete and therefore Banach.
    \end{thm}

    \begin{proof}[Idea of Proof]
        Given a Cauchy sequence $f_n$ in $C(K)$, define the limit pointwise. Then each $f_n$ is uniformly continuous and $f_n$ is uniformly Cauchy (so converges uniformly to $f$), so $f \in C(K)$.
    \end{proof}

    \begin{rmk}
        In general, the $\sup$ norm is not a norm on $C(X)$ but is a norm on $C_b(X)$.
    \end{rmk}

    We need compactness to obtain completeness. Define $C_c(X)$ be the set of continuous functions with compact support. 
    
    \begin{thm}
        Let $C_0(X)$ be the closure of $C_c(X)$ in $C_b(X)$. This is a Banach space.
        \begin{proof}[Sketch of Proof]
            Given a Cauchy sequence in $C_0(X)$, we need to show $f = \lim f_n$ is in $C_0(X)$. We use a diagonal argument.
            Each $f_n(X)$ is in $C_0(X)$, so each $f_n = \lim_{m \rightarrow \infty} f_{n,m}$ where each $f_{n,m}$ has compact support. Then we see that each $f = \lim_{n \rightarrow \infty} f_{n,n} \in C_0(X)$.
        \end{proof}
    \end{thm}

    We sau that $C_0(X)$ are the functions that ``vanish at infinity.'' Consider the prototype $C(K)$ is $C([a,b])$. 

    \begin{thm}[Weierstrass approximation theorem]
        Any function $f \in C([a,b])$ can be uniformly approximated by a polynomial. More precisely, the set of polynomials is dense in $C([a,b])$.
        \begin{proof}
            Without loss of generality, take $[a,b] = [0,1]$. Recall the binomial theorem $(a+b)^n = \sum_{k=0}^n \binom{n}{k}a^kb^{n-k}$. In particular $1 = \sum_{k=0}^n \binom{n}{k}x^k(1-x)^{n-k}$ for all $x,n$. Thus $f(x) = \sum_{k=0}^n f(x) \binom{n}{k}x^k(1-x)^{n-k}$. We then find the maximum of $x^k(1-x)^{n-k}$, which is $x = \frac{k}{n}$. Therefore, we define $B_nf = \sum_{k=0}^n f\left(\frac{n}{k}\right) \binom{n}{k}x^k(1-x)^{n-k}$.

            We show that $\norm{B_n f -f} \rightarrow 0$ as $n \rightarrow \infty$. Note $(B_nf - f)(x) = \sum_{k=0}^n \left( f\left(\frac{n}{k}\right) - f(x) \right) \binom{n}{k}x^k(1-x)^{n-k}$. Choose $\ep > 0$. Because $f$ is uniformly continuous, there exists $\delta$ such that $\abs{x-y} < \delta$ implies $\abs{f(x) - f(y)} < \ep$. Set $I(X) = \Set{k | \abs*{x-\frac{k}{n}} < \delta}$ and $J(x) = \Set{k | \abs*{x-\frac{k}{n}} \geq \delta}$.

            Then \[\abs{B_nf(x) - f(x)} \leq \sum_{I(x)} + \sum_{J(x)}  \abs*{f\left(\frac{n}{k}\right) - f(x)}  \binom{n}{k}x^k(1-x)^{n-k}. \] On $I(x)$, $\abs*{f(\frac{k}{n}) - f(x)} < \ep$, so \begin{align*}
                \norm*{B_nf - f} &\leq \sup_x \sum_{I(x)} \ep \binom{n}{k}x^k(1-x)^{n-k} + \sup_x \sum_{J(x)} 2 \norm{f} \binom{n}{k}x^k(1-x)^{n-k} \\
                & \leq \ep + 2 \norm{f} \sup_x \sum_{J(x)} \binom{n}{k}x^k(1-x)^{n-k}.
            \end{align*}
            Because $\delta$ has been fixed, on $J(x)$, $\left(x - \frac{k}{n}\right)^2 \geq \delta^2$, or $x^2 - 2 \frac{k}{n}x + \left(\frac{k}{n}\right)^2 \geq \delta^2$. Therefore \begin{align*}
                \sum_{J(x)} \binom{n}{k}x^k(1-x)^{n-k} &\leq \frac{1}{\delta^2} \left( \sum_{J(x)}\binom{n}{k}x^k(1-x)^{n-k} \right) \left( x^2 - 2 \frac{k}{n}x + \left(\frac{k}{n}\right)^2  \right),
            \end{align*}

            which is at most \[\frac{1}{\delta^2} \left( x^2 \sum_{J(x)} \binom{n}{k}x^k(1-x)^{n-k} - 2x \sum_{J(x)} \frac{k}{n} \binom{n}{k}x^k(1-x)^{n-k} + \sum_{J(x)} \left( \frac{k}{n} \right)^2 \binom{n}{k}x^k(1-x)^{n-k} \right).\]
            
            Therefore, \[ \sup_x \sum_{J(x)} \binom{n}{k}x^k(1-x)^{n-k} \leq \frac{1}{\delta^2} \sup \left[ x^2 B_n(1) - 2x B_n(x) + B_n(x^2) \right]. \] Now we compute $B_n(x), B_n(x^2)$.
            Note that $(x+y)^n = \sum \binom{n}{k} x^k y^{n-k}$. Differentiate with respect to $x$ and get $n(x+y)^{n-1} = \sum \binom{n}{k}kx^{k-1}y^{n-k}$. Therefore $n = \sum \binom{n}{k}kx^{k-1}(1-x)^{n-k}$. Therefore \[x = \sum \binom{n}{k}x^k(1-x)^{n-k} = B_n(x).\]

            Again, we now get $n(n-1)(x+y)^{n-2} = \sum \binom{n}{k}k(k-1)x^{k-2}y^{n-k}$. We attain that $n(n-1) = \sum \binom{n}{k}k(k-1)x^{k-2}(1-x)^{n-k}$. Multiplying through by $\left(\frac{n}{n}\right)^2$, we obtain that $x^2\left(1-\frac{1}{n}\right) = \sum \binom{n}{k} \frac{k^2}{n^2}x^k(1-x)^{n-k} - \frac{1}{n}\sum \binom{n}{k}\frac{k}{n}x^k(1-x)^{n-k} = B_n(x^2) - \frac{1}{n}B_n(x)$. Therefore, \[x^2-2xB_n(x)+B_n(x^2) = x^2-3x^2 + x^2 \left(1 - \frac{1}{n}\right) + \frac{x}{n} = \frac{1}{n}(x-x^2).\]

            We conclude that $\norm{B_nf - f} \leq \ep + \frac{2 \norm{f}}{\delta^2} \cdot \frac{1}{n} \sup{x-x^2}$.
        \end{proof} 
    \end{thm}

    \subsection*{Lecture 7 (Feb 12)}
    When Robin walked into lecture, Jared asked him if he knew what a circumgalactic medium was, and Robin said it sounds like the aether. Robin has also posted homework, which is due a week from today. We will finish the proof of the Weierstrass approximation theorem, which is in the notes for last time.

    Recall that in $\R^d$, $K$ is compact if and only if it is closed and bounded.

    \begin{defn}[Precompact]
        $M$ is precompact if its closure $\overline{M}$ is compact.
    \end{defn}

    Note that this means $M \subset \R^d$ is precompact if and only if it is bounded.

    However, observe that $C(K)$ is infinite-dimensional, so closed and bounded is not enough. Examples will come later. The concept that provides compactness in $C(K)$ is equicontinuity. This is an analogue of uniformity across the functions.

    \begin{defn}[Equicontinuity]
        Let $\mathcal{F}$ be a set of functions $X \rightarrow Y$. Then $\mathcal{F}$ is equicontinuous if given any $\ep > 0$ and $x_0 \in X$, there exists $\delta$ such that if $d(x,x_0) < \delta$, then $d(f(x), f(x_0)) < \ep$ for all $f \in \mathcal{F}$.
    \end{defn}

    \begin{defn}[Uniform Equicontinuity]
        $\mathcal{F}$ is uniformly equicontinuous if $\delta$ is independent of $x_0$ (Think the definition of uniform continuity or uniform convergence).
    \end{defn}

    \begin{thm}
        If $\mathcal{F}$ is equicontinuous on $K$ where $K$ is compact, and $\mathcal{F} \subset C(K)$, then $\mathcal{F}$ is uniformly equicontinuous.\footnote{Robin put his notes down here in a show of mathematical dominance.}
        \begin{proof}
            Suppose not. Then there exists $\ep$ such that for all $n$, there exist $x_n,z_n,f_n$ such that $d(x_n,z_n) < \frac{1}{n}$, but $d(f_n(x_n), f_n(z_n)) \geq \ep$. The goal is to show that $\mathcal{F}$ is not equicontinuous. 
            
            We know that $K$ is compact, so from $\{x_n\}$ we can extract $x_{n_k} \rightarrow x$. Then the $z_{n_k}$ also converge to $x$.\footnote{Robin checked his notes here, forfeiting his mathematical dominance.} However, because $d(f_n(x_n), f_n(z_n) \geq \ep)$ for all $n$, we must have either $d(f_n(x_n), f_n(x)) \geq \ep / n$ or $d(f_n(z_n), f_n(x)) \geq \ep / n$. This contracts equicontinuity.
        \end{proof}
    \end{thm}

    \begin{thm}[Arzela-Ascoli (1880s?)]
        Let $K$ be compact. A subset $\mathcal{F} \subset C(K)$ is compact if and only if it is closed, bounded, and equicontinuous.
        \begin{proof}
            There are three steps:
            \begin{enumerate}
                \item If $\mathcal{F}$ is unbounded, then it is not precompact.
                \item If $\mathcal{F}$ is precompact, then it is equicontinuous.
                \item If $\mathcal{F}$ is bounded and equicontinuous, then it is precompact.
            \end{enumerate}
            Now we prove each part:
            \begin{enumerate}
                \item This one is easy. Build a sequence $f_n$ such that $\norm{f_{n+1}} \geq \norm{f_n} + 1$. This sequence is not Cauchy, so there is no convergent subsequence.
                \item Assume $\mathcal{F}$ is precompact. Then write $\overline{\mathcal{F}} \subset \bigcup_{f \in \mathcal{F}} B_{\ep/3}(f)$. We can extract a finite subcover $\overline{\mathcal{F}} \subset \bigcup_{i=1}^n B_{\ep/3}(f_i)$. Because each $f_i$ is uniformly continuous, then there exists $\delta_i$ such that if $d(x,y) < \delta_i$, then $d(f_i(x), f_i(y)) < \ep/3$ for all $x,y \in K$. Choose $\delta = \min \delta_i > 0$. Now if $d(x,y) < \delta$, then for any $f \in \mathcal{F}$, choose $i$ such that $\norm{f-f_i} < \ep/3$. Therefore, $d(f(x), f(y)) \leq d(f(x), f_i(x)) + d(f_i(x), f_i(y)) + d(f_i(y), f(y)) < \ep$.
                \item Let $\mathcal{F}$ be bounded and equicontinuous. We will show that it is sequentially pre-compact. Given a bounded sequence, extract a Cauchy subsequence. Because $C(K)$ is complete, this implies sequential compactness. Let $\{f_n\} \subset \mathcal{F}$ be a bounded sequence. Because $K$ is compact, it has a countable dense subset $\{x_1, \ldots, x_n, \ldots\}$.
                
                Extract a subsequence $f_{1,n}$ of $f_n$ such that $f_{1,n}(x)$ converges. Then extract a subsequence $f_{2,n}$ of $f_{1,n}$ such that $f_{2,n}(x_2)$ converges. Then we extract a subsequence $f_{k+1}, n$ of $f_{k,n}$ such that $f_{k+1}(x_{k+1})$ converges. Continue this process to infinity. Now extract the diagonal sequence of functions $g_k = f_{k,k}$. We know that at each $x_i$, $g_k(x_i)$ is Cauchy. We will show that $g_k$ is uniformly Cauchy.

                We will use equicontinuity of $\mathcal{F}$. Because $K$ is compact, $\mathcal{F}$ is uniformly equicontinuous. Thus there exists $\delta$ such that if $d(x,y) < \delta$, then $d(g_k(x) - g_k(y)) < \ep/3$ for all $k$. Because $K$ is compact with $\{x_i\}$ dense, $K \subset \bigcup_{i=1}^{\infty} B_{\delta}(x_i)$. We extract a finite subcover $K \subset \bigcup_{j=1}^N B_{\delta}(z_j)$. For each $z_j$, the sequence $g_k(z_j)$ is Cauchy. Therefore, there exists $N_j$ such that if $k,\ell > N_j$, then $d(g_k(z_j), g_{\ell}(z_j)) < \ep/3$. Choose $N = \max N_j < \infty$. Then given any $x \in K$, for $k,\ell > N$, choose $z_j$ such that $d(x, z_j) < \delta$. Then $d(g_k(x), g_{\ell}(x)) \leq d(g_k(x), g_k(z_j)) + d(g_k(z_j), g_{\ell}(z_j)) + d(g_{\ell}(z_j), g_{\ell}(x)) < \ep$. \qedhere
            \end{enumerate}
        \end{proof}
    \end{thm}

    \subsection{Lecture 8 (Feb 14 $\heartsuit$)}
    Last time we saw that equicontinuity and boundedness implies precompactness, so what is equicontinuity?

    \begin{exm}[Not equicontinuous]
        Let $h_n$ be the skew hat function, which is piecesise linear and continuous with $h_n(\frac{1}{n}) = 1, h_n(\frac{1}{n \pm 1}) = 0$. Notice that the $\{h_n\}$ are not equicontinuous because $\norm{h_n - h_m} = 1$ for all $n \neq m$. Therefore this sequence has no convergent subsequence, so it is not precompact. However, this sequence is clearly bounded.
    \end{exm}

    \begin{exm}
        If $\mathcal{F}$ is a uniformly Lipshitz family, then it is equicontinuous. $\mathcal{F}$ is uniformly Lipshitz if there exists $K$ such that for all $f \in \mathcal{F}$, $\abs{f(x) - f(y)} \leq K\abs{x-y}$.
    \end{exm}

    We can weaken this statement, but not by much.

    \begin{exm}
        Suppose our $f$ are represented by power series of the form $f = \sum_{j=0}^{\infty} a_jx_j$. Then $f(x)-f(y) = \sum_j a_j(x^j-y^j)$. Recalling that $(x^j - y^j) = (x-y)(x^{j-1} + \cdots + y^{j-1})$, we see that \[f(x) - f(y) = (x-y) \sum_{j=0}^{\infty} a_j \sum_{k=0}^{j-1} y^kx^{j-1-k}.\] Then note that $\sum_{k=0}^{j-1}$ can only be bounded by $jL^{j-1}$ if $x,y \in [-L,L]$. To use the factorization, the only possible bound is \[\abs{f(x) - f(y)} \leq \abs{x-y}\sum_{j=1}^{\infty} \abs{a_j}jL^{j-1}.\] This morally looks like $\sup_{[-L,L]} \abs{f'(x)}$, so the moral is that we need some sort of derivative to claim precompactness.
    \end{exm}

    \begin{rmk}
        We will see later that functions with $k$ derivatives are precompactly contained in functions with $k-1$ derivatives. Thus we can find a convergent subsequence of a bounded sequence, but the limit can only be expected to be $C^{k-1}$.
    \end{rmk}

    \subsubsection{Differential Equations}

    \begin{exm}[Existence theorem for ODEs]
        Consider the ODE $\dot{u} =f(t,u)$ with initial condition $u(t_0) = u^0$. We will take $u \in \R^n$ and $f: \R^n \rightarrow \R^n$. We will explore the existence of solutions ot this equation. Assume that $f(t,u)$ is continuous.

        Recall Euler's method, which is to set $u(t+h) = u(t) + \int_t^{t+h} f(u,s) \d s$ and approximate the integral term, taking $n$ steps of constant size. Given $h$, we will build $u_h^0$ and then $u_h^{k+1} = u_h^k + hf(t_k, u_h^k)$. Then we take $u_h(t)$ to be the piecewise linear interpolation of these points.

        Now we extract a convergent subsequence from the $u_h$. We call $u_{\ep}(t)$ an $\ep$-approximate solution to the initial value problem if \begin{enumerate}
            \item $u_{\ep}(t_0) = u_0$;
            \item $u_{\ep}(t)$ is continuous, and it is differentiable at all but finitely many points of $I$;
            \item If $\dot{u}_{\ep}$ exists at $t$, then $\abs{\dot{u}_{\ep}(t) - f(t, u_{\ep}(t))} \leq \ep$.
        \end{enumerate}

        Then for $t \in (t_k, t_{k+1})$, we have $\abs{\dot{u}_h(t) - f(t,u_h)} = \abs{f(t_k, u_h^k) - f(t, u_h(t))}$. We will use continuity of $f$ to make this small:
        \begin{thm}
            There exists an interval $I$ and a continuously differentiable function $u:I \rightarrow \R^d$ such that the IVP holds in $I$.
        \end{thm}

        Note that the above is not a global existence theorem. Let $T_1$ be our initial guess for the time of existence. Choose $L > 0, T \leq T_1$ such that for each $u_{\ep}$ with $\abs{t-t_0} \leq T_1$ satisfies $\abs{u_{\ep}(t) - u_0} \leq L$. If $\abs{f} \leq M$, then set $T = \min\{T_1, L/M\}$. Call $R = \Set{(t,u) | \abs{t-t_0} \leq T, \abs{u-u_0} \leq L}$. Then $R$ is compact, so $f$ is uniformly continuous in $R$. Thus $u$ is uniforly Lipshitz continuous. Therefore $u_{\ep}$ is equicontinuous and bounded. By Arzela-Ascoli, we extract a convergent subsequence. Let $u(t) = \lim_{\ep_k \rightarrow 0} u_{\ep_k}(t)$. We know that $u(t) \in C[t_0-T, t_0+T]$. 

        We need to show that the limit is continuously differentiable and satisfies the IVP. We know that $u_{\ep}(t)$ satisfies \begin{align*}
            u_{\ep}(t) &= u_{\ep}(t_0) + \int_{t_0}^t \dot{u}_{\ep}(s) \d s \\
            &= u_0 + \int_{t_0}^t f(s, u_{\ep}(s)) \d s + \int_{t_0}^t \left(\dot{u}_{\ep}(s - f(s,u_{\ep}(s)))\right) \d s
        \end{align*}

        Now we see that $\abs*{\int_{t_0}^t \dot{u}_{\ep}(s) - f(s,u_{\ep}(s)) \d s} \leq \abs{t-t_0}\cdot K \cdot \ep$, which approaches $0$ by construction. Therefore $u(t) = u_0 + \int_{t_0}^t f(s,u(s)) \d s$. Because $f$ is continuous, then $\int f$ is differentiable and thus $u \in C^1$.
    \end{exm}
    
    We have the following theorem for these solutions.
    \begin{thm}
        Let $R = \Set{(t,u) | \abs{t-t_0} \leq T, \abs{u-u_0} \leq L}$ and suppose $\abs{f(t,u)} \leq M$ on $R$. If $\delta = \min\{T,L/M\}$. Then $u(t)$ solution is defined ans satisfies $\abs{u(t) - u_0} \leq L$ for all $t \leq \delta$ and if $f$ is Lipshitz in $u$ and uniform in $t$, then the solution is unique.
    \end{thm}

    \begin{exm}
        Consider the initial value problem $\dot{u} = u^{1- \alpha}, u(0) = 0$, where $\alpha > 0$. Then $u(t) = 0$ is a solution. Also we see that $\frac{\mathrm{d} u}{\mathrm{d} t} = u^{1-\alpha}$, so separating variables and integrating yields $u = (\alpha t)^{1/\alpha}$, which satisfies the initial condition. Therefore uniqueness fails.
    \end{exm}

    The first part of Theorem 69 is the statement of time of existence. While $f$ remains continuous, the only way for solutions to fail to exist is for them to ``escape to $\infty$,'' or to leave any compact set.

    \subsubsection{Digression}
    Consider a $4$-body problem. We consider gravitational attraction. We want to find out where solutions do not exist, where a solution is a continuous trajectory for each planet. The question is: how do solutions fail to be globally defined?
    \begin{enumerate}
        \item Collision: f has a singularity and loses continuity. For the $3$-body, problem, this is all;
        \item For the $4$-body problem, there are initial conditions for which solutions escape to $\infty$ in finite time!
    \end{enumerate}

    \begin{exm}
        Consider a $5$-body problem with two pairs of two overlapping elliptical orbits. Then the fifth body is an oscillator between the two pairs of elliptical orbits.
    \end{exm}

    \subsection{Lecture 9 (Feb 21)}%
    Last time we discussed Peano's proof of the existence of solutions to first order differential equations using the Euler approxumation. The idea is that we construct an equicontinuous sequence, so we extract a convergent subsequence, which is the solution.

    Robin then proceeded to tell a story about a time when he visited Hershey Park in Hershey, PA and went on a rollercoaster.\footnote{He has no idea why he said that.} This somehow relates to the five-body problem where solutions escape to infinity in finite time. There is a Cantor set of initial conditions under which planets escape to infinity in finite time. Jared asked him what that would mean physically, and Robin started talking about shockwaves, which he claims to do for a living. He argues that it is more physically relevant to study integral equations than differential equations.

    \subsubsection{Differential Equations Continued}%

    \begin{thm}
        Let $R$ be the rectangle $\Set{(t,u) | \abs{t-t_0} \leq T, \abs{u-u_0} \leq L}$. Let $f$ be continuous on $R$, so there exists $M$ such that $\abs{f(t,u)} \leq M$ on $R$. 
        \begin{description}
            \item[Time of existence] Let $\delta = \min \{T,L/M\}$. Then the solution $u(t)$ exists and stays in $R$ for $\abs{t-t_0} \leq \delta$, i.e. $\abs{u(t) - u_0} \leq L$ for all $t \in [t_0-\delta, t_0+\delta]$.
            \item[Uniqueness] If $f$ is Lipshitz in $u$ and uniformly continuous for $t \in R$, then the solution is unique for $\abs{t-t_0} \leq \delta$.
        \end{description}
        \begin{proof}
         \begin{description}
             \item[Time of existence] Define $D = \Set{\xi | 0 \leq \xi \leq \delta, \abs{u(t)-u_0} \leq L, \abs{t-t_0} \leq \xi}$. Clearly $0 \in D$, and also if $\xi \in D$ and $\xi' \in [0,\xi)$, then $\xi' \in D$. Therefore $D$ is an interval. $D$ is closed because $u$ is continuous. 
                 )
                 Also, $D$ is relatively open in $[0,\delta]$. If $\xi < \delta$ and $\xi \in D$, we have $f(t,u(t)) \leq M$ for $\abs{t-t_0} \leq \delta$. Therefore \[ \abs{u(t) - u(t_0)} \leq \abs*{\int_{t_0}^t f(s,u(s)) \d s } \leq M \xi < M \delta \leq L. \] Therefore there exists $\ep$ such that $\abs{u(t) - u_0} < L$ for $\abs{t-t_0} \leq \xi + \ep$. Therefore there exists a neighborhood of $\xi \subset D$. Therefore $D = [0,\delta]$. 
             \item[Uniqueness] Consider two solutions $u,v$. Then we see that \[\abs{u(t) - v(t)} \leq \abs{u_0-v_0} + \int_{t_0}^t \abs{f(s,u(s) - f(s,v(s)))} \d s \leq \abs{u_0-v_0} + K \int_0^t \abs{u(s) - v(s)} \d s, \] where $K$ is the unoform Lipshitz constant. Therefore set $w(t) = \abs{u(t) - v(t)}$. It satisfies the inequality $w(t) \leq w_0 + K \int_{t_0}^t w(s) \d s$ for $\abs{t-t_0} \leq \delta$. This is a linear integral inequality. We will apply ``Gronwall's Lemma.''

                 \begin{lem}[Gronwall]
                     If $w(t) \geq 0$ and $\varphi(t) \geq 0$ are continuous and defined on $[0,T]$, with $w_0 \geq 0$ ans satisfy \[ w(t) \leq w_0 + \int_0^t \varphi(s)w(s) \d s, \] then for $t \leq T$ we have $w(t) \leq w_0 e^{\int_0^t \varphi(s)\d s}$.
                 \end{lem}

                 Applying Gronwall here, $w(t) \leq w_0e^{K(t-t_0)}$ for all $\abs{t-t_0} \leq \delta$. In particular, if $w_0 = 0$, then $w(t) = 0$ for all $t$. \qedhere
        \end{description}           
        \end{proof}
    \end{thm}

    Note that we have proved continuous dependence on the initial conditions for short times. In particular, chaotic systems are predictable for a short time, after which they become unpredictable.

    \begin{proof}[Proof of Gronwall]
        Set $u(t) = w_0 + \int_0^t \varphi(s)w(s) \d s$. Then we know that $w(t) \leq u(t)$ for all $t$. We differentiate to obtain $\dot{u}(t) = \varphi(t)w(t) \leq \varphi(t)u(t)$. Also if $w_0 > 0$, then $u(0) = w_0 > 0$. We see that $\dot{u}(t) \geq 0$, so $u(t) \geq 0$. We can now take logs to obtain for all $t$, \[\frac{1}{u(t)} \dot{u}(t) \leq \varphi(t), \] or $(\log u(t))^{\dot{}} \leq \varphi(t)$. Therefore we integrate to find that $\log u(t) - \log u(0) \leq \int_0^t \varphi(s) \d s$. Therefore $u(t) \leq u(0)e^{\int_0^t \varphi(s) \d s}$. If $w_0 = 0$, then we can't do this. Instead replace $w_0$ with $\ep$ and let $\ep \to 0$.
    \end{proof}

    \begin{proof}[Alternative existence proof for differential equations]
        This proof uses Picard iteration. Remember the initial equation was $\dot{u}(t) = f(t,u(t)), u(t_0) = u_0$. We integrate to obtain \[u(t) = u_0 + \int_0^t f(s, u(s)) \d s.\] This is an implicit nonlinear integral equation, but there is a natural iteration, which is Picard iteration.

        Set $u_{k+1}(t) = u_0 + \int_0^t f(s,u_k(s)) \d s$. This is explicit and so convergence of the iteration to $u(t) \in C(K)$ yields a solution. Then abstractly: Let $X$ be a complete metric space (think $X = C([t_0-\delta, t_0+\delta])$). Then define the operator $T:X \to X$ given by $Tu(t) = u_0 + \int_0^t f(s,u(s)) \d s$. Then $u(t)$ solves the ODE if and only if it is a fixed point of $T$. We look for an easy condition on a map which yields a fixed point.

        \begin{defn}[Strict Contraction]
            A nonlinear map $T:X \to X$ is a strict contraction if there exists $\rho < 1$ such that for all $x,y \in X$, $d(Tx,Ty) \leq \rho d(x,y)$. Then we use the following theorem:
        \end{defn}

        \begin{thm}[Contraction Mapping Theorem]
            A strict contraction on a complete metric space has a unique fixed point.
        \end{thm}

        We show that the map $T$ given above is a strict contraction. Then we see that \[\norm{(Tu)(t) - (Tv)(t)} \sup \abs*{\int_{t_0}^t K \abs{u(s) - v(s)} \d s} \leq K \delta \norm{u-v}, \] so if $K \delta < 1$, this is a contraction.
    \end{proof}

    \begin{proof}[Proof of contraction mapping theorem]
        For uniqueness, note that if there are two fixed points, then that contradicts the definition of a strict contraction.

        For existence, just iterate. For any $x_0$, set $x_{k+1} = T x_k$. We show that this sequence is Cauchy. After that, because $X$ is complete, the sequence converges, and its limit is a fixed point by continuity. To see that the sequence is Cauchy, this is simply because geometric series converge (this is left as an exercise because I don't feel like typing).
    \end{proof}

    \section{Banach Spaces}
    
    \subsection{Lecture 10 (Feb 26)}
    We began the proof using Picard iteration last time, and Robin is leaving that as an exercise. We begin the discussion of Banach spaces, which are covered in chapters $5$ and $12$ of the textbook. Robin talked about proving estimates. We will add arithmetic structure to our metric space.

    Recall the definition of a normed vector space $X$:

    \begin{defn}[Normed Vector Space]
        A normed vector space $X$ is a vector space satisfying:
        \begin{enumerate}
            \item $\norm{u} \geq 0$, $\norm{u} = 0 \Rightarrow u = 0$;
            \item $\norm{\alpha u} = \abs{\alpha} \norm{n}$;
            \item $\norm{u + v} \leq \norm{u} + \norm{v}$.
        \end{enumerate}
    \end{defn}

    The extra structure here is linearity, which allows us to translate and scale, so the entire topology is determined by the unit ball.

    \begin{defn}[Banach Space]
        A Banach space is a complete normed vector space.
    \end{defn}

    \begin{exm}
        The first example is $\R^n$ with any $p$-norm: \[\norm{(x_1, \ldots, x_n)}_p = \left( \sum_{i=1}^n \abs{x_i}^p \right)^{1/p}.\] The power of $1/p$ is forced on us by the second axiom, and the triangle inequality is not at all obvious. Then for $p = \infty$, define $\norm{x}_{\infty} = \max \abs{x_i}$, which is the limit of the $p$-norms.

        We examine $B_1(0)$ for different $p$-norms, which has boundary $\abs{x}^p + \abs{y}^p = 1$. For $p = 1$ we get the cross-polytope, $p=2$ the circle, and for $p = \infty$ we get the square. For $p<1$ we lose convexity of the unit ball, but for $p>1$ convexity holds, which is equivalent to the triangle inequality.

        Note that the unit balls are nested for $1 \leq p \leq \infty$ but inclusion of unit balls reverses inequalities in sizes of norms.
    \end{exm}
    
    \begin{exm}
        We have already seen that $C(K)$ with the $\sup$ norm is Banach. However, note that $C^p(K)$ is not closed with respect to the $L^{\infty}$ norm. The reason is that the norm cannot control derivatives.

        To make $C^p(K)$ Banach, we include the derivatives in the norm:
        \[\norm{f}_{C^p(K)} = \norm{f}_{\infty} + \norm{f'}_{\infty} + \cdots + \norm{f^{(p)}}_{\infty}.\] An alternative is to use $\norm{f}_{\infty} + \norm{f^{(p)}}_{\infty}$. The two norms are equivalent.
    \end{exm}

    \begin{defn}
        $(X, \norm{\cdot}_1), (X, \norm{\cdot}_2)$ are equivalent as Banach spaces if the norms are equivalent (they bound each other above and below up to some constant). Clearly equivalent norms generate identical topologies.
    \end{defn}

    \begin{exm}
        Ww can extend norms on $\R^n$ to infinite dimensions: Let $\ell^p(\mathbb{N})$ be the set of sequences such that $\sum_{i} \abs{x_i}^p$ is finite. This is Banach with the $p$-norm.
    \end{exm}

    \begin{exm}
        If $K \subset \R^n$, $L^p(K) = \Set{f | \int_K \abs{f}^p < \infty}$ is Banach with norm the integral to the $1/p$ power.
    \end{exm}

    \begin{exm}
        If $(X, \mu)$ is a finite measure space, then we can define $L^p(X, \mu) = \Set{f | \int \abs{f}^p \d \mu < \infty}$. Then this space with the obvious norm is a Banach space.

        Note that the assumption that the space has finite measure is a convenience to ensure completeness. Otherwise, we must avoid issues of support ``escaping to $\infty$.''
    \end{exm}

    Our goal is to show that $L^p(X, \mu)$ is a Banach space. Before we do this, we introduce Sobolev spaces:

    \begin{defn}[Sobolev Space]
        Define the Sobolev space to be:
        \[W_{k,p}(X,\mu) = \Set{f | \abs*{\sum_{j=0}^p f^{(j)(x)}} \in L^p}.\]
        Set the norm to the only possible norm.

        Note that here we are cheating because we don't know if the derivatives exist. We use a notion of weak derivative.
    \end{defn}

    \begin{thm}
        $L^p(X,\mu)$ is a Banach space.
        \begin{proof}
            Non-negativity (up to sets of measure $0$) and scaling are obvious. We need to prove the triangle inequality. Recall Jensen's inequality:
            \begin{mdframed}[style=default]
                \begin{thm}[Jensen's Inequality]
                Define the average $\gen{f}_{\mu}$ to be \[\gen{f}_{\mu} = \frac{1}{\mu(X)} \int_X f(x) \d \mu.\] Then if $\varphi: \R \to \R$ is convex, $\varphi(\gen{f}_{\mu}) \leq \gen{\varphi \circ f}_{\mu}$.
                \end{thm}
                Proof of Jensen was on the final exam last semester, so it is omitted.
            \end{mdframed}
            Now recall Cauchy-Schwarz:
            \begin{mdframed}[style=default]
                \begin{thm}[Cauchy-Schwarz]
                    If $f,g$ are square-integrable, then \[ \int \abs{fg} \d \mu \leq \norm{f}_2 \norm{g}_2. \]
                \end{thm}
            \end{mdframed}
            
            \subsection{Lecture 11 (Feb 28)}
            
            
            H\"older's inequality generalizes this to $p$:
            \begin{mdframed}[style=default]
                \begin{thm}[H\"older's Inequality]
                Let $1 \leq p \leq \infty$ and $p'$ be the H\"older conjugate $p/(p-1)$. Then if $f \in L^p, g \in L^{p'}$, then $fg \in L^1$ and
                \[ \int \abs{fg} \d \mu \leq \norm{f}_p \norm{g}_{p'} \]
            \end{thm}
            \begin{proof}
                If $p=1$ or $p = \infty$ this is obvious. We can assume that $g \neq 0$ everywhere. The idea is to use Jensen. We will apply Jensen with $\varphi(z) = z^p$, which is convex. Set $\nu(A) = \int_A \abs{g}^{p'} d \mu$, so $\nu(X) = \norm{g}_{p'}^{p'}$. Note that by construction, $\abs{g}^{p'}$ is the Radon-Nikodym derivative $\mathrm{d}\nu/\mathrm{d}\mu$. Therefore, by Jensen, we have
                \[\left( \frac{1}{\nu(X)} \int_X h \d \nu \right)^p \leq \frac{1}{\nu(X)} \int_X h^p \d \nu.\] Taking roots and simplifying, we obtain
                \begin{align*}
                    \int h\abs{g}^{p'} \d \mu &\leq \left( \norm{g}_{p'}^{p'} \right)^{1-1/p} \left( \int h^p \abs{g}^{p'} \d \mu \right)^{1/p} \\
                    \int \abs{fg} \d \mu &\leq \norm{g}_{p'} \left( \int \abs{f}^p \abs{g}^{p(1-p') \abs{g}^{p'}} \d \mu \right)^{1/p} \\
                                         &= \norm{f}_p\norm{g}_p
                \end{align*}
                if we set $h = \abs{f}\abs{g}^{1-p'}$.
            \end{proof}
            \end{mdframed}

            \begin{mdframed}[style=default]
                \begin{cor}
                    Assume $1 \leq q \leq p \leq \infty$. Then $L^1(X,\mu) \supset L^q(X,\mu), \supset L^p(X,\mu) \supset L^{\infty}(X,\mu)$.
                \end{cor}
                \begin{proof}
                    Set $\frac{1}{p} + \frac{1}{r} = \frac{1}{q}$. Thus $\frac{1}{p/q} + \frac{1}{r/q} = 1$. Therefore if $f \in L^p$, then $f^{q} \in L^{p/q}$. Also, since $\mu(X) < \infty$, $1 \in L^{\alpha}$ for all $\alpha$, and in particular $1 \in L^{r/q}$. Now we can write
                    \begin{align*}
                        \norm{f}_q^q &= \int \abs{f}^q \cdot 1 \d \mu \\
                                     &\leq \norm{\abs{f}^q}_{p/q} \norm{1}_{r/q} \\
                                     &= \left( \int \abs{f}^p \d \mu \right)^{q/p} \cdot \mu(X)^{q/r} \\
                                     &\leq \norm{f}_p^{q} \cdot \mu(X)^{q/r}.
                    \end{align*}
                    Thus $f \in L^q$.
                \end{proof}
            \end{mdframed}

            Now we can prove Minkowski's inequality:
            \begin{mdframed}[style=default]
                \begin{thm}[Minkowski's inequality]
                    If $f,g \in L^p$, then $f + g \in L^p$ and $\norm{f+g} \leq \norm{f} + \norm{g}$.
                \end{thm}
                \begin{proof}
                    Firstly, note that $\abs{f+g}^p \leq (\abs{f} + \abs{g})^p \leq 2^p\max(\abs{f}^p, \abs{g}^p) \leq 2^p(\abs{f}^p + \abs{g}^p) < \infty$, so $f+g \in L^p$.

                    This means that $\abs{f+g}^p \in L^1$, so $\abs{f+g}^{p-1} \in L^{p'}$. Therefore
                    \[\norm{\abs{f+g}^{p-1}}_{p'} =\left( \abs{f+g}^p \d \mu \right)^{1/p'} = \norm{f+g}_p^{p/p'} = \norm{f+g}_p^{p-1}.\]
                    Therefore, 
                    \begin{align*}
                        \norm{f+g}_p^p &= \int \abs{f+g}^p \d \mu \\
                                       &= \int \abs{f+g}^{p-1}\abs{f+g} \d \mu \\
                                       &\leq \int \abs{f+g}^{p-1}\abs{f} \d \mu + \abs{f+g}^{p-1}\abs{g} \d \mu \\
                                       &\leq \norm{\abs{f+g}^{p-1}}_{p'}(\norm{f}_p + \norm{g}_p) \\
                                       &= \norm{f+g}_p^{p-1}(\norm{f}_p + \norm{g}_p),
                    \end{align*}
                    which gives the triangle inequality.
                \end{proof}
            \end{mdframed}
            This implies that $L^p(X,\mu)$ is Banach. To prove $L^p$ is Banach, use the completeness of $L^1$. If $\{f_k\}$ is Cauchy in $L^p$, then $\abs{f_k}^p$ is Cauchy in $L^1$, so we take the $\frac{1}{p}$ power of the limit.
        \end{proof}
    \end{thm}

    An alternative approach is to use Young's inequality $ab \leq \frac{a^p}{p} + \frac{b^q}{q}$, which is not Young's convolution inequality.\footnote{Robin did not prove this inequality either.}

    We return to an abstract Banach space $X$. Unless explicitly stated otherwise, assume that $X$ is separable. Then we say $X$ has a Schauder basis if:

    \begin{defn}[Schauder basis]
        $X$ has a Schauder basis if it has a countable set $x_1, \ldots, x_n$ such that for all $x \in X$, we can approximate $X$ by $\sum c_nx_n$.
    \end{defn}

    \begin{exm}
        The standard example is Fourier series, which can be uniquely approximated by $f(x) = \sum a_n \cos(nx) + \sum b_n \sin(nx)$.
    \end{exm}

    Consider a linear operator $T$.
    \begin{defn}[Bounded Linear Operator]
        $T$ is bounded if there exists $X$ such that for all $x \in X$, we have $\norm{Tx}_Y \leq K \norm{x}_X$.
    \end{defn}

    The set of bounded linear operators is a normed vector space with the norm given by \[\norm{T}= \inf\Set{K | \norm{Tx} \leq K \norm{x}}\]. This is equivalent to $\sup_{\norm{x} = 1} \norm{Tx}$.

    Write $B(X,Y)$ to mean the space of bounded operators. If $X,Y$ are Banach, so is $B(X,Y)$.

    \begin{thm}
        $T$ is bounded if and only if it is continuous at $0$.
        \begin{proof}
            It is easy to see that boundedness implies continuity. To prove the converse, suppose not. if $T$ is unbounded, then for all $N$, there exists $x_N$ such that $\norm{Tx_N} > N \norm{x_N}$. Take $x_n$ to have length $1/N$. Then $\norm{Tx_N} > 1$, so $T$ is not continuous at $0$.
        \end{proof}
    \end{thm}
    We actually ended on time today, and Robin made a remark about me putting it in the notes.

    \subsection{Lecture 12 (Mar 5)}
    Robin apologizes for not assigning enough homework. He will assign homework this week, which will be due after break. There will be no midterm, unlike last semester.\footnote{Robin went into how he doesn't see the point of mowing lawns.}
    
    Let $T:X \to Y$ be a linear map of normed vector spaces. We see that the kernel and image are clearly subspace, and if $T$ is bounded, then $\mathrm{ker} T$ is closed (by continuity). $X$ is the domain of $T$. Note that changing the domain changes the operator. In a finite dimensional space, all linear operators are bounded (the closed unit ball is compact).

    \begin{lem}
        The closed unit ball is compact if and only if $\mathrm{dim} X < \infty$.
        \begin{proof}
            In finite dimensions, the closed unit ball is closed and bounded, so it is compact.

            In infinite dimensions, this follows from Riesz's Lemma. We will find a sequence $x_n$ in $\overline{B}$ which has no convergent subsequence. Take $r = \frac{1}{2}$. For each $n$, apply Riesz's Lemma to $X_{n-1} = \mathrm{Sp}\{x_1, \ldots, x_{n-1}\}$. $X_{n-1}$ is not dense, so there exists $x_n$ such that $\norm{x}_n = 1$ and $d(x_n, X_{n-1}) \geq \frac{1}{2}$. Then no subsequence of $\{x_n\}$ can converge.
        \end{proof}
    \end{lem}

    \begin{lem}[Riesz]
        If $X \subset Y$ is a subspace, then if $X$ is not dense, then for all $0 < r < 1$, there exists $y \in Y$ such that $\norm{y} = 1$ and $d(y,X) \geq r$.
        \begin{proof}
            First choose $y_1 \not\in \overline{X}$. Let $R = \inf_{x \in X} \norm{x-y_1} > 0$. We will scale and translate to get a unit vector. Choose $\ep > 0$ and choose $x_1 \in X$ such that $\norm{x_1-y_!} < R+\ep$. Now set $y = \frac{y_1-x_1}{\norm{y_1-x_1}}$. Now we see that
            \begin{align*}
                \inf_{x \in X} \norm{x-y} &= \inf_{x \in X} \norm*{x - \frac{y_1}{y-x_1} + \frac{x_1}{y_1-x_1}} \\
                                          &= \inf_{x \in X} \norm*{\frac{x-y_1+x_1}{\norm{y_1-x_1}}} \\
                                          &\geq \frac{R}{R+\ep}.
            \end{align*}
            Now choose $\ep$ small enough such that $\frac{R}{R+\ep} \geq r$.
        \end{proof}
    \end{lem}
   
    \begin{exm}
        In $C_{\infty}[0,1]$ with any norm, the derivative is unbounded (take $e^{\lambda t}$ for example). Similarly, if we consider $\{\sin(nt), \cos(nt)\}$ as a Schauder basis on $C_{\mathrm{per}[0,2\pi)}$,
        $D \sin(nt) = n\cos(nt)$.
    \end{exm}

    \begin{exm}
        On $C[0,1]$, there are two common integral operators:
        \begin{description}
            \item[Fredholm] $f(x) \mapsto Kf = \int_0^1 f(y)k(x,y)\d y$, where $k(x,y)$ is some kernel. $K$ is bounded in the $\sup$ norm if $\max_{0 \leq x \leq 1} \int_0^1 \abs{k(x,k)}\d y$ is finite. We see that\[ \norm*{\int_0^1} f(y)k(x,y)\d y = \sup_x \abs*{\int f(y) k(x,y) \d y} \leq \norm{f} \sup_x \int \abs{k(x,y)} \d y. \]
            \item[Volterra] $f(x) \mapsto Vf(x) = \int_0^x f(y) k(x,y) \d y$. Again $\norm{V} = \sup_x \int_0^1 \abs{k(x,y)} \d y$. Note that the range of $V$ is not closed because $Vf$ is smoothing $Vf \in C^1$.
        \end{description}
    \end{exm}

    \begin{exm}
        Let $\ell_{\infty}(\mathbb{N}) = \{(x_1, \ldots, x_n, \ldots)\}$ with the sup norm. The right shift $S$ and left shift $T$ are clearly bounded. $S$ is not onto and the image of $S$ is closed. Also, $\mathrm{Ker} T = \{(x,0,\ldots)\}$. An example of this is integrals and derivatives of power series with basis $\left\{ \frac{x^n}{n!} \right\}$.
    \end{exm}

    \begin{thm}[BLT]
        If $X$ is a normed vector space, $Y$ Banach, and $M$ a dense linear subspace of $X$, then any bounded operator $T:M \to Y$ has a unique extension $\overline{T}:X \to Y$ such that $\overline{T}x = Tx$ for all $x \in M$ and $\norm{\overline{T}} = \overline{T}$.
        \begin{proof}
            Clearly we define $\overline{T}$ to be the limit $\overline{T}x = \lim Tx_n$ where $x_n \to x$. Clearly this is well-defined (otherwise boundedness is contradicted). Also note that $\overline{T}$ is clearly bounded by boundedness of $T$ and that $\overline{T}x = Tx$ for $x \in M$. By continuity, this is linear.

            To prove uniqueness, let $\overline{T}, \widetilde{T}$ extend $T$. Then by continuity, they must be equal ($M$ is dense).
        \end{proof}
    \end{thm}
    \begin{rmk}
        The above theorem allows us to pass to the limit from, for example, $C^{\infty}$.
    \end{rmk}
    \begin{defn}[Linear Functional]
        A linear functional on $X$ is a bounded linear map $x \to \R$.
    \end{defn}

    Because $\R$ is complete, the set of such functionals is complete whether or not $X$ is complete. We call this space $X^*$, the dual space of $X$.

    \subsection{Lecture 13 (Mar 7)}
    Robin began class questioning the point of shoveling snow. He says that it is necessary so you can drive your car away from your house.

    Let $\mathcal{B}(X,Y)$ be the space of bounded linear maps $X \to Y$. A natural question is: when can we invert $T$? Robin drew a diagram and mentioned something about us not taking his linear algebra class. In $\R^n$, we have the transpose $T^t: Y \to X$ (note that this is not possible unless we have an inner product).

    \begin{thm}[Rank-Nullity]
        For finite dimensional vector spaces $\R^n$, we have $\mathrm{dim}(\mathrm{im}\ T) = \mathrm{dim}(\mathrm{im}\ T^t)$, $\mathrm{ker}\ T \perp \mathrm{im}\ T^t$.
    \end{thm}

    The restriction of $T$ is invertible as a map $T: (\mathrm{ker}\ T)^{\perp} \to \mathrm{im}\ T$. In finite dimensions, a necessary algebraic condition for solvability of $Tx = y$ is $\mathrm{ker}\ T = 0$. This is not enough. In our context, we want a bounded (continuous) inverse.

    \begin{thm}
        A bounded map $T: X \to Y$ of Banach spaces has a bounded inverse if and only if $\mathrm{ker}\ T = 0$ and $\mathrm{im}\ T$ is closed in $Y$.\footnote{Proof of this is not given.}
        \begin{proof}
            One of the directions is easy. To prove the converse, we use the open mapping theorem.

            Assuming the open mapping theorem, we note that $T$ open implies $T^{-1}$ continuous.
        \end{proof}
    \end{thm}


    \begin{thm}[Open mapping theorem]
        A bounded surjective map is open.
        \begin{proof}[Sketch of Proof]
            We show that $T(B_1(0))$ is open. Since $T$ is surjective, write $Y = \bigcup_{n=1}^{\infty} T(B_n(0))$. Then apply the Baire Category Theorem. Therefore there exists $n$ such that $\overline{T(B_n(0))}$ has an interior. Eventually, we prove that there exists $\ep$ such that $B_{\ep}(0) \subset T(B_n(0))$.
        \end{proof}
    \end{thm}
    
    \begin{cor}
        If $V$ is a vector space with norms $\norm{-}_1$ and $\norm{-}_2$ such that $\norm{v}_1 \leq K \norm{v}_2$ and if $V$ is complete with respect to both norms, then the norms are equivalent.
    \end{cor}

    Note that $\mathcal{B}(X,Y)$ has more structure, namely composition. In particular, $\mathcal{B}(X)$ is a Banach algebra. This allows us to consider nonlinear functions of $T \in \mathcal{B}(X)$.

    \begin{exm}
        Some examples are exponentials and the Neumann series. To define the exponential, if $T \in \mathcal{B}(X)$, then we can define $e^{\alpha T}$ for any $\alpha$ by a power series. It is easy to see that the series is absolutely convergent. Some properties of the exponential are that $\frac{\mathrm{d}}{\mathrm{d}\ \alpha} e^{\alpha T} = T e^{\alpha T}$ and $e^T e^S = e^{T+S}$ is $TS=ST$.

        The Neumann weries is defined as the inverse of the operator $I-T: X \to X$. This is given by the usual geometric series.
    \end{exm}

    In general, in the Bamach algebra $\mathcal{B}(T)$, we can extend  $f(T)$ for any given function $f: \R \to \R$ with convergent power series.

    \begin{defn}
        An operator $T: X \to Y$ is compact if for any bounded set $B \subset X$, the image $TB$ is precompact in $Y$.
    \end{defn}

    Sometimes, $\norm{T}$ is too strong of a norm for our purposes. For $T \in \mathcal{B}(x,y)$, $T_n$ converges uniformly if $\norm{T_n - T} \to 0$. We will try to weaken this to broaden the class of convergent sequences. The first obvious weakening is pointwise convergence.

    \begin{exm}
        Consider $\ell^p(\mathbb{N})$. Then let $P_n$ be projection onto the first $n$ components. We see that $\norm{P_n - P_m} = 1$ for all $n \neq m$, so $P_n$ is not Cauchy. However, $P_n \to \mathrm{id}$ pointwise.
    \end{exm}

    \begin{defn}[Weak convergence]
        We weaken this notion further by declaring $T_n \to T$ weakly, written $T_n \rightharpoonup T$ if for each $x \in X$, $T_nx \rightharpoonup Tx$ in $Y$. Thus we use weak convergence in $Y$ to define weak convergence in $\mathcal{B}(X,Y)$.
    \end{defn}

    \begin{defn}[Weak convergence in $Y$]
        $y_k \rightharpoonup y$ weakly in $Y$ if for all $f \in Y^*$, $f(y_k) \to f(y)$ in $\R$.\footnote{This is analogous to convergence in measure.}
    \end{defn}

    \subsection{Lecture 14 (Mar 19)}

    We continue our discussion of dual spaces. Let $X$ be a normed vector space and $Y$ be Banach. Then $\mathcal{B}(X,Y)$, the space of bounded linear maps $X \to Y$ inherits completemenss from $Y$. Two of the most important $Y$ are $X$ and $\R$. Denote the algebraic dual space of $X$ by $X'$ and the topological dual by $X^*$. Note that $X^* \subseteq X'$ with the inclusion an equality when $X$ is finite-dimensional and a strict inclusion when $X$ is infinite-dimensional. 

    \begin{lem}
        $X^*$ and $X$ are isomorphic when $X$ is finite dimensional.
        \begin{proof}
            Choose a basis $e_1, \ldots, e_d$ of $X$. Then any $\varphi \in X^*$ satisfies $\varphi(x) = \varphi \left( \sum x_ke_k \right) = \sum x_k \varphi(e_k)$, so every $\varphi$ is determined by its values on the basis. Choose a dual basis $w_1, \ldots, w_d$ given by $w_k(e_j) = \delta_{jk}$. Clearly the $w_k$ are lienarly independent and any $\varphi$ can be written as \[\varphi = \sum \alpha_j w_j\] by the expression for $\varphi(x)$ we computed above. The isomorphism is obvious now that they have the same dimension.
        \end{proof}
    \end{lem}

    \begin{rmk}
        Note that this isomorphism is basis-dependent.
    \end{rmk}

    \begin{exm}
        If $X$ is infinite dimensional, start with a countable independent set $\{e_1, \ldots, e_j \ldots \}$. Extend to a Hamel basis. Now given $f \in X$, write $f = \sum c_h h$ and note $\varphi_j$ the usual way. Now form $\varphi = \sum n \varphi_n$. Then $\varphi \in X'$ but $\varphi$ is unbounded.
    \end{exm}

    \begin{exm}[Dual of $L^p(X, \mu)$]
        The answer is easy to find but hard to prove. Use H\"older's inequality. If $p'p'$ are conjugate and if $f \in L^p, g \in L^{p'}$, then $\norm{fg}_1 \leq \norm{f}_p \norm{g}_{p'}$. Thus we expect that $(L^p)^* = L^{p'}$ for $1 \leq p < \infty$.

        \begin{proof}[Sketch]
            The first step is to show the equality $\norm{g}_{p'} = \sup_{\norm{f}_p = 1} \norm{fg}_1$. Take $f(x) = \abs{g(x)}^{p'-1} \mathrm{sgn}(g(x)) \in L^p$. Scale down with the appropriate powers and checking the desired equality is left as an exercise.
            
            The next step is to show that $(L^p)^* \subset L^{p'}$. Start with $\varphi \in (L^p)^*$ and assume $X$ has finite measure. Note that $\chi_E \in L^p$ for all $p$. For measurable $E$, define $\nu(E) = \varphi(\chi_E)$. Then $\norm{\chi_E}_p = \mu(E)^{1/p}$. Also $\nu(E) \leq \norm{\varphi}_{(L^p)^*} \norm{\chi_E}_p = K \mu(E)^{1/p}$, where $K = \norm{\varphi}_{(L^p)^*}$.
            
            We show that $\nu$ is a signed measure and that $\nu \ll \mu$. It is clear that $\nu$ is finitely additive. Similarly, if $E = \cup_{j \geq 1} E_j$ a disjoint union, write $E = \cup_{j=1}^n E_j \cup F$. Then $\chi_E = \sum_{1}^N \chi_{E_j} + \chi_F$. Therefore \[ \nu(E) = \varphi(\chi_E) = \sum_{j=1}^N \varphi(\chi_{E_j}) + \varphi(\chi_F) = \sum_{j=1}^N \nu(E_j) + \nu(F).\] From above, $\nu(\overline{E}) \leq K \mu(\overline{E})^{1/p}$, so if $p < \infty$, $\nu \ll \mu$. Also since $\mu(X) < \infty$, $\mu(F) \to 0$ as $N \to \infty$. Thus $\nu$ is countably additive.

            Now we use Radon-Nikodym to obtain that there exists $g \in L^1(X)$ such that for all $E$ measurable, $\nu(E) = \int_E g \d\mu$. Thus $\nu(E) = \varphi(\chi_E) = \int_E g \d\mu = \int g \chi_E \d\mu$. Now $\varphi$ extends to simple functions and then to measurable (and integrable) functions by taking limits. Therefore if $\varphi(f) = \int fg \d\mu$ for $f \in L^1$. Then if $f \in L^p$, \[\abs*{\int fg \d \mu} = \varphi(f) \leq \norm{\varphi}_{(L^p)^*} \norm{f}_{L^p}. \] Together with the first staement, this implies $g \in L^{p'}$ and that the norms are equal.
        \end{proof}
    \end{exm}

    \begin{rmk}
        The assumption $p < \infty$ is essential, and in fact for $X$ of finite measure, any finite measure $\nu$ on $X$ with finite variation yields a bounded linear functional: $\gen{\nu,f} = \int f \d \nu$. For general $X$, we require Radon measures.
    \end{rmk}
    
    So far, we have $X, X^*$ and $X^*$ is complete. Note that $L^2$ is self-dual and we know that $L^p$ is reflexive for $1 < p < \infty$: $L^p \simeq (L^p)^{**}$. It is easy to show in general that $X \subset X^{**}$: $\gen{x,\varphi} = \varphi(x) = \gen{\varphi, x}$. Clearly this is bounded because $\abs{\gen{\varphi,x}} \leq \norm{\varphi}_{X^*} \norm{x}_X$, so $x$ is a bounded linear operator on $X^*$.

    Linear functionals are very useful geometrically and analytically. We use them to define weak convergence and weak derivatives. A question is: can $X^*$ separate a convex set from its exterior? if $K$ is convex and $p \not\in \overline{K}$, is there some $\ell \in X^*$ such that $\ell(x) \geq a$ for $x \in X$ and $\ell(p) < a$?

    \subsection{Lecture 15 (Mar 21)}

    Robin apologizes for not returning our homework. He's so organized.
    
    We work in a vector space. Let $K$ be bounded convex and $z \notin K$. We want to find a linear functional separating $z$ from $K$ such that $\ell(x) < a$ for $x \in K$ and $f(z) \geq a$. Note $\ell(x) = a$ defines a ``hyperplane.'' Thus there is no loss of generality in taking $0 \in K$. We will build a ``Minkowski gauge function'' by scaling $K$ around $0$. 
    
    We will define $p_k(x) = \inf \{ r \geq 0 \mid x \in rK\}$. Then $p_k(x) \leq 1$ if $x \in K$ and $p_k(x) \geq 1$ if $x \notin K$. Also $p_k(tx) = tp_k(x)$ for all $t \geq 0$ and $p_k(\alpha x + (1-\alpha)y) \leq \alpha p_k(x) + (1-\alpha)p_k(y)$ for all $x,y$ and $\alpha \in [0,1]$.

    A function $p$ satisfying the last two conditions is called sublinear. The idea of Hahn-Banach is to extend a linear functional defined on the ray through $z$ to a linear functinal on all of $X$, still dominated by $p$.

    \begin{thm}[Hahn-Banach]
        Let $X$ be a vector space and $p: X \to \R$ sublinear. Then let $Y \subseteq X$ be a subspace and $\ell$ is a linear functional on $Y$ such that $\ell(y) \leq p(y)$ for all $y \in Y$. Then $\ell$ extends to a linear functional $\varphi$ on all of $X$, such that $\varphi(x) \leq p(x)$ for all $x \in X$ and $\varphi(y) \leq \ell(y)$ for all $y \in Y$.
        \begin{proof}
            First we show that we can extend by one dimension, then we close the process using the axiom of choice. Take $z \notin Y$. Let $\widetilde{Y} = \mathrm{Sp}\{Y,z\}$. Then $\varphi$ is defined on $Y$, so we need $\varphi(x)$. Note $\varphi(az + y) = a \varphi(z) + \varphi(y)$ for $y \in Y$. It is not enough to set $\varphi(z) = p(z)$ and extend by linearity. Choose two points $y_1, y_2 \in Y$ and $\alpha,\beta \in \R$. Consider
        \begin{align*}
            \beta \varphi(y_1) + \alpha y_1 &= \varphi(\beta y_1 + \alpha y_2) \\ 
                                            &= (\alpha + \beta)\varphi \left( \frac{\beta}{\alpha + \beta} y_1 + \varphi(\alpha){\alpha + \beta} y_2 \right) \\
                                            &= (\alpha + \beta) p \left( \frac{\beta}{\alpha + \beta} (y_1 + \alpha z) + \frac{\alpha}{\alpha + \beta} (y_2 - \beta z) \right) \\
                                            &\leq \beta p(y_1 + \alpha z) + \alpha p(y_2 - \beta z).
        \end{align*}
        Now divide by $\alpha \beta$ and separate: \[ \frac{1}{\alpha} (-p(y_1 + \alpha z) + \varphi(y_1)) \leq \frac{1}{\beta} (p(y_2 - \beta z) - \varphi(y_2)). \] In conclusion, there exists $a \in \R$ such that \[ \sup_{\alpha,y} \frac{1}{\alpha} (-p(y_1 + \alpha z) + \varphi(y_1)) \leq a \leq \inf_{\beta, y_2} \frac{1}{\beta} (p(y_2 - \beta z) - \varphi(y_2)). \] Now set $\varpi(z) = a$ and extend by linearity. Undoing this, we get $\varphi(x) \leq p(x)$ for all $x \in \widetilde{Y}$.

        For the second step, use Zorn's lemma. Consider all extensions $( \widetilde{Y}, \varphi_{\widetilde{Y}} )$. Partially order them in the obvious way (subspace and restriction agrees). Now consider a chain $\{(\widetilde{Y}_{\alpha}, \varphi_{\widetilde{Y}_\alpha})\}$. Then take $\widetilde{Y} = \cup_{\alpha} \tilde{Y}_{\alpha}$. This is an upper bound for the chain. Therefore the set of extensions has a maximal element. Call it $( \hat{Y}, \varphi_{\hat{Y}} )$. Clearly this must be all of $X$ (otherwise apply step $1$).
        \end{proof}
    \end{thm}

    \begin{rmk}
        We can prove Hahn-Banach without requiring the axiom of choice. 
    \end{rmk}

    \begin{cor}
        Let $X$ be a normed vector space and $Y \subset X$, and suppose $\lambda \in Y^*$.  Then there exists $\Lambda \in X^*$ extending $\lambda$ such that $\norm{\Lambda}_{X^*} \leq \norm{\lambda}_{Y^*}$.
        \begin{proof}
            Set $p(x) = \norm{\lambda}_{Y^*} \norm{x}$ and apply Hahn-Banach
        \end{proof}
    \end{cor}

    \begin{cor}
        For $y \in X$, there exists $\Lambda \in X^*$ such that $\lambda(y) = \norm{\Lambda}_{X^*} \norm{y}$. 
        \begin{proof}
            Set $Y = \mathrm{Sp}(y), \lambda(y) \norm{y}$. Apply the previous corollary.
        \end{proof}
    \end{cor}

    \begin{cor}
        Let $Z \subset X$ be a subspace. Let $y \in X$ with $d(y,z) = \delta > 0$. Then there exists a linear functional $\Lambda \in X^*$ satisfying $\norm{\Lambda} \leq 1$. $\Lambda(y) = \delta$, and $\lambda(z) = 0$ for all $z \in Z$.
    \end{cor}

    \begin{thm}
        If $X^*$ is separable, then $X$ is separable.
        \begin{proof}
            If $\{\lambda_n\}$ is dense in $X^*$, for each $n$, choose $x_n \in X$ such that $\norm{x_n} = 1$ and $\abs{\gen{\lambda_n, x_n}} \geq \norm{\lambda_n}/2$. Consider \[\mathcal{D} = \Set{\sum_{n=1}^N r_nx_n | r_n \in \Z}. \] We show $\overline{\mc{D}} = X$.

            Suppose not. Then there exists $y \in X \setminus \overline{\mc{D}}$ with $d(y, \mc{D}) = \delta > 0$. By Corollary 121, there exists $\Lambda$ such that $\Lambda(x) = 0$ for all $x \in \mc{D}$. Because $\Lambda \in X^*$, there exists a subsequence $\lambda_{n_k} \to \Lambda$. Then \[\norm{\Lambda - \lambda_{n_k}} \geq \norm{\gen{\Lambda - \lambda_{n_k}, x_{n_k}}} = \norm{\gen{-\lambda_{n_k}, x_{n_k}}} \geq \norm{\lambda_{n_k}}/2\]. This implies that $\Lambda = 0$, which is a contradiction.
        \end{proof}
    \end{thm}

    \begin{rmk}
        The converse of the above is not necessarily true. For example $L^1$ is separable but $(L^1)^* = L^{\infty}$ is not separable.
    \end{rmk}

    \section{Hilbert Spaces}
    \subsection{Lecture 16 (Mar 26)}

    We will add more structure to our vector spaces. We revert to complex vector spaces. These are not conceptually harder, just computationally harder. Our inspiration comes from Pythagoras. The Euclidean distance comes from the dot product in $\R^n$.

    \begin{defn}[Inner Product]
        An inner product in a complex vector space is a ``sesquilinear form,'' i.e. a map $(\ ,\ ): X \times X \to \C$ such that:
        \begin{enumerate}
            \item $(x, \alpha y + \beta z) = \alpha(x,y) + \beta(x,z)$ where $\alpha, \beta \in \C$;
            \item $(x,y) = \overline{(y,x)}$ (Hermitian Symmetric);
            \item $(x,x) \geq 0$. In particular, $(x,x) \in \R$;
            \item $(x,x) = 0$ if and only if $x = 0$.
        \end{enumerate}
    \end{defn}

    We observe that the inner product is conjugate linear in the first variable and that sesquilinearity is needed to ensure non-negativity. We refer to $X$ equipped with $(\ , \ )$ as an inner product space.

    \begin{defn}[Hilbert Space]
        A Hilbert space is a complete inner product space.
    \end{defn}

    \begin{exm}
        Consider complex matrices $\C^{m \times n}$ equipped with the inner product $(A,B) = \mathrm{tr}(A^HB)$. This is called the ``Hilbert-Schmidt norm.''
    \end{exm}

    \begin{exm}
        Consider $L^2(X,\mu)$ with functions to $\C$. Then set $(f,g) = \int_X \overline{f(x)}g(x) \d\mu$. This is an inner product space, but is not Hilbert in general. We simply take the completion.
    \end{exm}

    \begin{exm}
        Now work with $C^{(k)}(X)$ complex valued, $k$-times differentiable $f$. Set \[(f,g) = \sum_{j=0}^k \int_X \overline{f^{(j)}(x)}g^{(j)}(x) \d\mu; \] and set $H^k(X)$ to be the completion of $C^k(X)$.
    \end{exm}

    \begin{lem}
        An inner product $(\ ,\ )$ induces a norm via $\norm{x} = \sqrt{(x,x)}$.
        \begin{proof}
            Linearity is obvious ($\sqrt{\lambda \overline{\lambda}} = \abs{\lambda}$) for all $\lambda \in \C$ and nonnegativity is also trivial. The triangle inequality follows from Cauchy-Schwarz.
        \end{proof}
    \end{lem}

    \begin{thm}[Cauchy-Schwarz]
        For all $x,y \in X$, we have $\abs{(x,y)} \leq \norm{x} \norm{y}$ with inequality if and only if $x,y$ are linearly dependent.
        \begin{proof}
            Set $\lambda = (x,y)/\norm{y}^2$. Then
            \begin{align*}
                0 &\leq \norm{x - \lambda y}^2 \\
                  &= (x,x) - \lambda(y,x) - \overline{\lambda}(x,y) + \lambda \overline{\lambda}(y,y)\\
                  &= \norm{x}^2 - \frac{\abs{(x,y)}^2}{\norm{y}^2} - \frac{\abs{(x,y)}^2}{\norm{y}^2} - \frac{\abs{(x,y)}^2}{\norm{y}^2} \\
                  &= \norm{x}^2 - \frac{\abs{(x,y)}^2}{\norm{y}^2}. \\
            \end{align*}
            This implies the desired result.
        \end{proof}
    \end{thm}

    \begin{thm}[Parallelogram Law]
        In any inner product space, $\norm{a+b}^2 + \norm{a-b}^2 = 2(\norm{a}^2 + \norm{b}^2)$.
    \end{thm}

    \begin{rmk}
        In fact, a norm on a vector space is induced from an inner product if and only if the parallelogram law holds.
    \end{rmk}

    \begin{cor}
        $L^p$ is not Hilbert unless $p=2$.
    \end{cor}

    The law of cosines holds in any inner product space. More importantly, $a,b$ are orthogonal if and only if $(a,b) = 0$.

    \begin{thm}
        Let $A \subset H$ be a subset of a Hilbert space. Define $A^{\perp} = \{x \mid x \perp a \text{ for all }a \in A\}$. Then $A^{\perp}$ is a closed linear subspace of $H$.
        \begin{proof}
            It is clear that this is a subspace. Closedness is a consequence of continuity of the inner product.
        \end{proof}
    \end{thm}

    A partial converse is:
    \begin{thm}
        If $M$ is a closed linear subspace of $H$. Then:
        \begin{enumerate}
            \item For all $x \in X$, there exists a unique $y \in M$ such that $\norm{x-y} = \min_{z \in M} \norm{x-z}$;
            \item $y$ is the unique element such that $(x-y) \perp M$.
        \end{enumerate}
        \begin{proof}
            Recall that in finite dimensions, we simply use a sequence of projections. Set $\delta = \inf_{z \in M} \norm{x-z}$. There exists a sequence $z_n \in M$ such that $\norm{x-z_n} \leq \delta + 1/n$. We will show that the limit exists and that this element is unique.
        \end{proof}
    \end{thm}

    \subsection{Lecture 17 (Mar 28)}

    We work in a Hilbert space $H$ with $M$ a closed subspace. We conclude the proof of Theorem 135 from last time. 

    \begin{proof}[Proof of Theorem 135, continued]
        We set $\delta = d(x,M)$. There exists a sequence $z_n \in M$ such that $\norm{x-z_n} \leq \delta + 1/n$. We will show that the limit exists and that this element is unique. Consider the triangle with vertices $x, z_n, z_m$. We will use the parallelogram law:
        \begin{align*}
            \norm{z_n-z_m}^2 &= 2 \norm{z_n-x}^2 + 2\norm{z_m-x}^2 - 4\norm{x-\overline{z}}^2 \\
                             &\leq 2 (\delta + 1/n)^2 + 2 (\delta+1/m)^2 - 4 \delta^2 \\
                             &= 4 \delta/n + 4 \delta/m + 2/n^2 + 2/m^2.
        \end{align*}
        Therefore the sequence is cauchy and converges, so $y$ exists.

        To show uniqueness, suppose there are two such points $y_1, y_2$. Then $\norm{y_1-y_2}^2 = 2 \norm{x-y_1}^2 + 2 \norm{x-y_2}^2 - 4\norm{x-\overline{y}}^2 \leq 2 \delta^2 + 2 \delta^2 - 4 \delta^2 = 0$.

        We show that $(x-y,z) = 0$. For all $\lambda \in \C$, we have that $\norm{x-y}^2 \leq \norm{x-y-\lambda z}^2 = \norm{x-y}^2 + \abs{\lambda}^2 \norm{z}^2 - 2 \mathrm{Re} \lambda (x-y,z)$. Let $(x-y,z) = re^{i\theta}$. Then setting $\lambda = te^{-i\theta}$, we get that $0 \leq t^2\norm{z}^2 - 2tr$. Dividing by $t^2$, we have $\frac{2r}{t} \leq \norm{z}^2$ for all nonzero $t$, which is only possibleif $r=0$.

        To show uniqueness, suppose there is another such $y'$. We know that $\norm{x-y'} \geq \norm{x-y}$. Using the parallelogram law, we know $\norm{x-y}^2 = \norm{x-y}^2 + \norm{y-y'}^2$ because $(x-y, y-y') = 0$. However, we see $\norm{x-y}^2 = \norm{x-y'^2} + \norm{y-y'}^2$ and thus $y=y'$.
    \end{proof}
    
    \begin{rmk}
        We will reserve the direct sum $\oplus$ for an orthogonal direct sum because if $M \subset X$ is closed with $X$ Banach, there may not be a closed space $N$ such that $v=m+n$, $m \in M, n \in N$. In a Hilbert space, for any set $X$, we can write $H = S^{\perp} \oplus S^{\perp\perp}$.
    \end{rmk}

    \begin{defn}[Orthogonal Subset]
        $U$ is an orthogonal subset if $u \perp v$ for all $u,v \in U$.
    \end{defn}

    \begin{defn}[Orthonormal Subset]
        $U$ is orthonormal if $U$ is orthogonal and $\norm{u} = 1$ for all $u \in U$.
    \end{defn}
    
    \begin{exm}
        Consider $2 \pi$-periodic functions, or functions on $\mathrm{T}^n = (S^1)^n$. Then $e^{in \cdot x}$ for $n \in \Z^d, x \in \mathrm{T}^d$ are orthogonal in $L^2(\mathrm{T}^d)$. This forms a Fourier basis.
    \end{exm}

    \begin{exm}
        We can also consider quasi-periodic functions. $f$ is quasi-periodic if it is the sum of finitely many periodic functions. Let $X$ be the space of periodic functions of the form $\sum_{k=1}^n a_k e^{i\omega_kt}$. We can define an inner product on $X$ by $(f,g) = \lim_{T \to \infty} \frac{1}{2T} \int_{-T}^T \overline{f(t)} g(t) dt.$. We see $X$ is not complete. The completion of $X$ consists of $L^2$ almost periodic functions $f(t) = \sum_{k=1}^{\infty} a_ke^{i\omega_k t}$. Note that $e^{i\omega t}$ are orthogonal for $\omega \in \R$. Thus we have an uncountable orthogonal set and thus $\overline{X}$ is not separable.
    \end{exm}

    In general we have an uncountable basis. Abstractly, we must make sense of $\sum_{\alpha \in I} X_{\alpha}$ where $I$ could be uncountable. 

    \begin{defn}[Unconditional Convergence]
        In a Banach space, we'll say the unordered sum $\sum_{\alpha \in I}$ converges if the partial sums $S_J$ converge where $J$ is finite: there exists $J_{\ep}$ finite such that $\norm{S_j - x} < \ep$ for any $J \supset J_{\ep}$
    \end{defn}

    We define absolute convergence like we do for $\R$.

    \begin{lem}
        Absolute convergence implies unconditional convergence. Also, for all $n$ there exists $J_n$ finite such that $\norm{S_{J_n} - x} \leq 1/n$ and set $J = \cup J_n$, which is countable. We see that $x_{\alpha} = 0$ for all $\alpha \notin J$.
    \end{lem}

    \begin{defn}
        An unordered sum is Cauchy if for all $\ep > 0$ there exists $J_{\ep}$ such that $\norm{S_k} < \ep$ for all finite $K$ such that $K \subset I \setminus J_{\ep}$.
    \end{defn}

    \begin{lem}
        In a Banach space, an unordered sum converges if and only if it is Cauchy.
        \begin{proof}
            Suppose the sum converges. Then there exists $J_{\ep}$ such that $\norm{S_J - x} < \ep$ for any finite $J \supset J_{\ep}$. IF $K \subset I \setminus J_{\ep}$ is finite, set $J = J_{\ep} \cup K$. Then $\norm{S_k} = \norm{S_J - S_{J_{\ep}}} \leq \norm{S_J - x} + \norm{S_{J_{\ep}} - x} \leq 2 \ep$.

            Now suppose the sum is Cauchy. Choose $J_n$ increasing such that $\norm{S_K} < 1/n$ whenever $K \subset I \setminus J_n$ is finite. Then $\norm{S_{J_m} - S_{J_n}} < 1/n$, so $S_{J_n}$ is Cauchy. Thus it converges to some $x$. Now given $\ep$, choose $n$ such that $1/n < \ep/2$. Then set $J_{\ep} = J_n$, so if $J \supset J_{\ep}$ is finite, then $\norm{S_J - x} \leq \norm{S_J - S_{J_n}} + \norm{S_{J_n} - x} \leq 2/n < \ep.$
        \end{proof}
    \end{lem}

    \subsection{Lecture 18 (Apr 02)}
    Recall the definition of unconditional convergence. Our goal is Bessel.

    \begin{lem}
        If $U = \{u_{\alpha} \mid \alpha \in J \}$ is an orthogonal set in $H$, then $\sum_{\alpha \in I} u_{\alpha}$ converges unconditionally if and only if $\sum_{\alpha \in I} \norm{u_{\alpha}}^2 < \infty$, and in this case $\norm*{\sum u_{\alpha}}^2 = \sum \norm{u_{\alpha}}^2$.
        \begin{proof}
            For any finite set $J$, $\norm*{\sum_J u_{\alpha}}^2 = \sum_J \norm{u_{\alpha}}^2$. Therefore unconditional convergence is equivalent to Cauchy inplies that $\sum u_{\alpha}$ is Cauchy if and only if $\sum \norm{u_{\alpha}}^2$ is Cauchy in $\R$.
        \end{proof}
    \end{lem}

    \begin{cor}
        $\sum_I c_{\alpha} u_{\alpha}$ converges unconditionally in $H$ if and onlf if $c_{\alpha}=0$ for all but countable many $\alpha$.
    \end{cor}

    \begin{thm}[Bessel]
        If $U = \{u_{\alpha} \mid \alpha \in I\}$ is an orthonormal set in $H$, then
        \begin{enumerate}[label=(\alph*)]
            \item $\sum_{\alpha }\in I \abs{\gen{u_{\alpha,x}}}^2 \leq \norm{x}^2$ for all $x \in H$.
            \item $x_U \coloneqq \sum_{\alpha \in I} \gen{u_{\alpha},x} u_{\alpha}$ converges unconditionally for all $x \in H$l
            \item $x-x_U \in U^{\perp}$.
        \end{enumerate}

        \begin{proof}
            We must understand the sum in (b).
            \begin{align*}
                \norm*{x-\sum_J \gen{u_{\alpha},x} u_{\alpha}}^2 &= \gen{x-\sum_J \gen{u_{\alpha},x} u_{\alpha}, x-\sum_J \gen{u_{\alpha},x} u_{\alpha}} \\
                                                                 &= \gen{x,x} - \sum_{\beta \in J} \gen{u_{\beta},x}\gen{x,u_{\beta}} - \sum_{\alpha \in J} \overline{\gen{u_{\alpha},x}} \gen{u_{\alpha},x} + \sum_{\alpha,\beta} \overline{\gen{u_{\alpha},x}}\gen{u_{\beta},x}\gen{u_{\alpha},u_{\beta}} \\
                                                                 &= \norm{x}^2 - \sum_{\alpha \in J} \abs{\gen{u_{\alpha},x}}^2
            \end{align*}
            Therefore, for any finite $J$, we have \[\sum_J \abs{\gen{u_{\alpha},x}}^2 = \norm{x}^2 - \norm*{x - \sum_J \gen{u_{\alpha},x}}^2 \leq \norm{x^2}\]
            This implies that $\sum_{\alpha \in I} \abs{\gen{u_{\alpha,x}}}^2 $ converges if and only if
            $\norm{\sum_{u_{\alpha},x}u_{\alpha}}^2$ converges.

            If $v \in U$, then \[\gen{c,x-x_U} = \gen{v,x - \sum \gen{u_{\alpha},x} u_{\alpha}} = \gen{c,x} - \sum_{\alpha \in I} \gen{u_{\alpha},x} \gen{v,u_{\alpha}} = 0. \]
        \end{proof}
    \end{thm}
    
    We will write $[U]$ for the closed linear span of $U$ for an orthonormal set $U \subset H$.

    \begin{thm}
        $[U] = \Set{ \sum c_u u | \sum \abs{c_u}^2 < \infty }$.
    \end{thm}

    \begin{thm}
        Let $U$ be an orthonormal subset of $H$. Then the following are equivalent:
        \begin{enumerate}[label=(\alph*)]
            \item $\gen{u_{\alpha},x} = 0$ for all $\alpha \in I$ only if $x=0$;
            \item $x = \sum_{\alpha \in I} \gen{u_{\alpha},x} u_{\alpha}$ for all $x \in H$;
            \item $\norm{x}^2 = \sum \abs{\gen{u_{\alpha},x}}^2$ for all $x \in H$;
            \item $[U] = H$;
            \item $U$ is a maximal orthonormal set in $H$.
        \end{enumerate}
        We call such a $U$ an orthonormal basis for $H$.
        \begin{proof}
            \begin{description}
                \item[(a) implies (b)] Note that $U^{\perp} = \{0\}$. Bessel implies that $x=x_U$.
                \item[(b) implies (c)] In the proof of Bessel.
                \item[(c) implies (d)] (c) states that $x \in [U]$. Thus $[U]^{\perp} - 0$, so $H \subset [U]$.
                \item[(d) implies (e)] If there exists $v$ such that $v \perp U$, then since $v \in H$, $v= \sum c_{\alpha} u_{\alpha}$ for some $c_{\alpha}$ such that $\sum \abs{c_{\alpha}}^2 < \infty$. Since $\gen{v,u_{\alpha}} = 0$ we must have $c_{\alpha} = 0$ and thus $v=0$.
                \item[(e) implies (a)] This is obvious.
            \end{description}
        \end{proof}
    \end{thm}

    We conclude that if we have an orthonormal basis, any $\gen{f,g}$ can be componentwise.

    \begin{thm}
        Any Hilbert space has an orthonormal basis.
        \begin{proof}
            Use Zorn's lemma. Consider the collection $\{U\}$ of orthonormal subsets ordered by inclusion. Given a chain, the union of all sets in the chain is an upper bound. Thus the collection has a maximal element. By Theorem 149, it is an orthonormal basis.
        \end{proof}
    \end{thm}

    \subsubsection{Adjoints}
    We return to Banach spaces. Let $A:X \to Y$ be a bounded linear map. We want to define a transpose. Recall the dual $X^* = \{\ell:X \to \R \text{ bounded linear } \}$. Thus $\gen{\ell,x}$ is given by evaluation. Then given a map $A:X \to Y$ we define an adjoint $A^+: Y^* \to X^*$ by $\gen{A^+ \lambda,x} = \gen{\lambda, Ax}$. We see that $A^+$ is linear and bounded and satisfies $\norm{A^+} = \norm{A}$.

    Recall rank-nullity. Then analogous statements hold for $A,A^+$. We introduce some notation: Given a subset $V \subset X$, the annihilator $N(V)$ is the subspace of $X^*$ such that $f \in N(V)$ if and onlf if $\gen{f,x} = 0$ for all $x \in V$. Then the range of $A$ is closed in $Y$ because $A$ is bounded. We want that $R(A) = N(A^+)^{\perp}$. Thus $R(A) = \mathrm{Ker}(A^+)^{\perp}$.
        
    If $y \in R(A) \subset Y$, then $y=Ax$ for some $x$. Under the identification $Y \subset Y^{**}$, 
    we have $R(A) \subset \mathrm{ker}(A^+)^{\perp}$.

    \subsection{Lecture 19 (Apr 04)}
    Last time we defined the adjoint on a Banach space. Robin is going to do a lot of handwaving as he talks about integration by parts. To motivate this discussion, consider $L^p$ (our spaces have not yet been properly defined). We consider $C_c^{\infty}$ the space of compactly supported $C^{\infty}$ functions on $\R$. These are dense in all interesting spaces and vanish at the boundary.

    Now define norms which include derivatives: $X = \overline{C}_c^{\infty}$ in the norm $\norm{u}_p + \norm{u'}_p$. Then $Y = L^p$ and our map is $D$, which is differentiation. We see that differentiation is linear and that $\norm{Du}_Y = \norm{u'}_{L^p} \leq \norm{u}_X$. We attempt to find the adjoint using $(L^p)^* = L^q$. Then the adjoint of $D$ satisfies $\gen{D^* \psi, u} = \int D^* \psi \cdot u \d x$, which must be \[\gen{\psi, Du} = \int \psi \cdot Du \d x = - \int (D \psi) u \d x.\] We conclude that $D^*$ should be $-D$, whatever this means. Note the domain of $D^*$ should be bigger than $L^q$. 

    \begin{rmk}
        Our couplings used to define adjoings are in some sense based on the $L^2$ inner product.
    \end{rmk}

    In a Hilbert space $H$, we have a natural pairing, which is given by the inner product. We would lise to define the adjoint of $A:H \to H$ by $(A^{\dag}x,y) = (x,Ay)$.\footnote{Note that Robin used $+$ last time, $*$ this time, and $\dag$ most recently.} This is consistent because any Hilbert space is self-dual.

    \begin{thm}[Riesz Representation Theorem]
        Let $H$ be a Hilbert space. Then for all $\varphi \in H^*$ there exists a unique $y \in H$ such that $\varphi = (y,-)$.
    \end{thm}

    We develop some machinery to prove Riesz. Let $X$ be a vector space. Then $P:X \to X$ is a projection is $P^2=P$. We can enforce an orthogonal projection if $(x,Py) = (Px,y)$. In a Hilbert space, $\norm{P} = 1$. For a proof, consider the inner product of $Px$ with itself. Define the reflection $\mc{R}f$ by $\mc{R}f(x) = f(-x)$. Then $\frac{I\pm\mc{R}}{2}$ are projections onto even and odd parts of $f$, respectively.

    \begin{proof}[Proof of Riesz Representation]
        If $\varphi = 0$ then we take $y=0$. Thus assume $\varphi \neq 0$. Then there exists $z \in H$ such that $\varphi(z)=0$ and $z \in (\mathrm{ker}P)^{\perp}$. Define $P:H \to H$ by $Px = \varphi{x} \cdot \frac{z}{\varphi(z)}$. It is clear that $P$ is a projection. Also note that $P(y) = 0$ if and only if $\varphi(y) = 0$. Thus $H = \mathrm{ker} \varphi \oplus \mathrm{Ran} P$, which is an orthogonal direct sum.

        Writing $x=w+\alpha z$, we see that $\varphi(x) = \varphi(w) + \alpha \varphi(z)$. Thus we want $y=\beta z$ such that $\varphi(x) = (y,x)$. We see that $\beta = \frac{\overline{\varphi(x)}}{\norm{z}^2}$. Then for all $x$, we can calculate that $(y,x) = \varphi(x)$.

        To show uniqueness, note that $(y-y_1,x) = 0$ for all $x$ if and only if $y=y_1$.
    \end{proof}

    \subsection{Lecture 20 (Apr 09)}
    Last time we proved that a Hilbert space is self-dual. Define $I: H^* \to H$ by the mapping given by the Riesz Representation Theorem. Adjoints in Hilbert spaces can now be defined using the inner product and we can define the adjoint by $(x,Ay) = (A^*x,y)$. We want to generalize the rank-nullity theorem. Because $(z,Ax) = (A^*z,x)$, then $A^*z = 0$ implies $z \perp \mathrm{ran} A$.

    \begin{thm}
        Let $A: H \to H$ be linear. Then $\overline{\mathrm{ran}(A)} = (\mathrm{ker}(A^*))^{\perp}$ and $\mathrm{ker} A = (\mathrm{ran}(A^*))^{\perp}$.

        \begin{proof}
            If $x \in \mathrm{ran} A$, then $x \perp \mathrm{ker}(A^*)$. If $x \in \mathrm{ran}(A)^{\perp}$, then $0 = (x,Ay) = (A^*x,y)$ for all $y$. Thus $x \in \mathrm{ker}A^*$. However we know that the closure of $\mathrm{ran}(A)$ contains $(\mathrm{ker} A^*)^{\perp}$. This gives the first part.

            For the second part, apply the first part to $A^{ ** } = A$.
        \end{proof}
    \end{thm}

    This theorem provides a necessary solvability condition for the abstract equation $Ax = y$, which is that $y$ is orthogonal to the kernel of $A^*$. One context for this is partial differential equations. We take a PDS and look for perturbations of some known solution: $u^{\ep} = u_0 + \ep u_1 + \ep^2 u_2 + \cdots$. Then we collect powers of $\ep$, which typically leads to separate PDEs for $u_1, u_2, \ldots$. The goal is to approximate the solution by $u_1$. We need $u_2$ to have some bounded solution, which is where we write the solvability condition.

    The second application is the relation between uniqueness and existence. Suppose that the adjoint problem $A^*x = y$ has a unique solution. Then $\mathrm{ker}A^* = 0$ Thus by the theorem, if $A$ has closed range, then $Ax=y$ has a solution for any $y$.

    \begin{lem}
        If $A^*x = y$ has a unique solution and if $\norm{Ax} \geq c \norm{x}$ for some $c>0$, then $Ax=y$ has a solution for all $y$.
    \end{lem}

    \begin{defn}
        $A:H \to H$ has the Fredholm alternative if either:
        \begin{enumerate}
            \item $Ax=0,A^*x=0$ implies $x=0$, and both $Ax=y$ and $A^*x = y$ have unique solutions;
            \item $A^*=0$ and $A^*x=0$ have nontrivial finite-dimensional solution spaces of the same dimension, and $Ax=y$ has a solution iff $y \perp z$ for all $z$ such that $A^*z = 0$, and the same holds for $A^*x=y$.
        \end{enumerate}
    \end{defn}

    \begin{defn}
        $A$ is a Fredholm operator if $A:H \to H$ and
        \begin{enumerate}
            \item $\mathrm{ran}A$ is closed;
            \item Both $\mathrm{ker}A$ and $\mathrm{ker}A^*$ are finite dimensional.
        \end{enumerate}
        Define the index of $A$ as $\mathrm{dim}\ \mathrm{ker} A - \mathrm{dim}\ \mathrm{ker}A^*$.
    \end{defn}

    \begin{prop}
        If $A$ is Fredholm and $K$ is compact, then $A+K$ is Fredholm.
    \end{prop}

    We will now discuss self-adjoint operators.
    \begin{defn}
        $A$ is self-adjoint if $A^* = A$.
    \end{defn}
    Self-adjoint operators satisfy the spectral theorem.

    \begin{lem}
        If $A$ is self-adjoint, then:
        \begin{enumerate}
        \item $\norm{A} = \sup_{\norm{x} = 1} (x,Ax)$;
        \item $\norm{A^*A} = \norm{A}^2$, and if $A$ is self-adjoint, then $\norm{A^2} = \norm{A}^2$;
        \end{enumerate}
    \end{lem}

    \begin{thm}[Spectral Theorem, compact case]
        Let $A: H \to H$ be compact and self-adjoint on $H$. Then there is an orthonormal basis of eigenvectors: The non-zero eigenvalues form a countable set of reals, and we can write $A = \sum_k \lambda_k P_k$ where each $P_k$ is an orthogonal projection onto the finite-dimensional eigenspace corresponding to $\lambda_k$. The series converges in operator norm.
    \end{thm}

    \subsection{Lecture 21 (Apr 11)}
    We consider the spectral theorem again. Recall the definition of a Fredholm operator. Note that $AA^*|_{\mathrm{ran}A}$ is invertible, and so is $A^*A|_{\mathrm{ran}A^*}$. If $A$ is self-adjoint, then the three conditions coincidem and the only requirement is that the kernel is finite-dimensional. In particular, if $A$ is invertible, it is Fredholm.

    Recall the definitions of eigenvalue and eigenvector.
    \begin{prop}
        If $A$ is self-adjoint, then 
        \begin{enumerate}
            \item All eigenvalues are real.
            \item Eigenvectors corresponding to different eigenvalues are orthogonal.
        \end{enumerate}
    \end{prop}

    Most of what we still will hold for normal operators, but we state them for self-adjoint operators.

    \begin{defn}[Normal Operator]
        An operator is normal if it commutes with its adjoint.
    \end{defn}

    \begin{exm}
        Skew-adjoint operators ($A = -A^*$) are normal.
    \end{exm}

    \begin{lem}
        Let $A:H \to H$ be self-adjoint. Then $\norm{A} = \sup_{\norm{x} = 1} (x,Ax)$.
        \begin{proof}[Sketch of Proof]
            Let $\alpha$ be the supremum. Then $\alpha \leq \norm{A}$ follows easily. Then note that $\norm{A} = \sup \abs{(y,Ax)}$, where $y,x$ have norm $1$. We can use the parallelogram law and choose $y$ by a factor of $e^{i\varphi}$.
        \end{proof}
    \end{lem}

    \begin{defn}[Resolvent]
        Define the resolvent of $A$ as $\rho(A) = \{z \in \C \mid A-zI \text{ is invertible } \}$.
    Note that because $A-zI$ is bounded, the inverse is also bounded.
    The spectrum of $A$ is $\sigma(A) = \C \setminus \rho(A)$.
    \end{defn}

    There are two cases for $\lambda \in \sigma(A)$:
    \begin{enumerate}
        \item $A-\lambda I$ is Fredholm. Then there is a finite eigenspace and thus $\lambda$ is an eigenvalue of finite multiplicity. The discrete spectrum is $\sigma_{dis}(A) = \{z \mid A-zI \text{ not invertible, but Fredholm } \}.$ For each such $z$, there are a finite number of eigenvalues that span a finite dimensional space $E_{\lambda}$, and $P_{\lambda}$ is the projection onto that space.
        \item $A-\lambda I$ is not Fredholm. Define the essential spectrum $\sigma_{ess}(A) = \{z \in \C \mid A-zI \text{ is not Fredholm } \}$. We know that
            \begin{enumerate}
                \item $\sigma_{ess}(A) \subset \sigma(A)$;
                \item $\sigma_{ess}$ is closed in $\C$;
                \item If $K$ is self-adjoint and compact, then $\sigma_{ess}(A) = \sigma_{ess}(A+K)$.
            \end{enumerate}
    \end{enumerate}

    \begin{thm}
        The following are equivalent:
        \begin{enumerate}
            \item $\lambda \in \sigma_{ess}(A)$.
            \item Weyl's Criterion: there exists a sequence $\psi_k$ in $H$ with $\norm{\psi_k} = 1$ such that $(a-\lambda I) \psi_k \to 0$, but $\{\psi_k\}$ has no convergent subsequence.
            \item $\mathrm{dim}(\mathrm{ker}(A-\lambda I)) = \infty$, or there exist $\sigma_n \in \sigma(A)$ such that $\sigma_n \to \lambda$.\footnote{You should regard this morally as poles and essential singularities.}
        \end{enumerate}
    \end{thm}

    \begin{rmk}
        Robin believes that if $A$ is normal, then there is no essential spectrum.
    \end{rmk}

    \section{Fourier Series}
    Consider $f:\R \to \C$ periodic with period $2 \pi$. We can regard $f: \mathbb{T} \to \C$. This becomes a Hilbert space with $(f,g) = \int_{\T} \overline{f(x)}g(x) dx$. The Fourier basis is the orthonormal basis $\{e_n(x)\}$ where $e_n(x) = \frac{e^{inx}}{\sqrt{2 \pi}}$. To get a hilbert space, we set $L^2(\T) = \overline{C(\T)}$ in the corresponding $L^2$ norm.

    \begin{prop}
        $\{e_n(x)\}$ is a complete orthonormal basis for $L^2(\T)$.
    \end{prop}

    We conclude that any function $f \in L^2(\T)$ can be written \[f = \sum_{n \in \Z} a_n e^{inx},\] where $a_n = (e^{inx},f)$. This sum converges provided $\sum \abs{a_n}^2 < \infty$. Using Euler's identity, we can write $f(x)$ in the real Fourier sine/cosine series \[f(x) = \sum_{n \geq 0} a_n \cos(nx) + \sum_{n \geq 0} b_n \sin(nx).\]
    We have $a_n = \frac{1}{\pi} \int_0^{2\pi} f(x) \cos(nx) d x$ and $b_n = \int_0^{2\pi} f(x) \sin(nx) dx$. Also $a_0 = \frac{1}{2\pi} \int_0^{2\pi} f(x) dx$. The sine and cosine series are particularly nice for even and odd functions. A property of $e_n$ plays very nicely with the derivative and is in fact an eigenfunction of the differentiation operator.

    \subsection{Lecture 22 (Apr 16)}
    We continue with our discussion of Fourier series. We will end lecture by solving a PDE. For simplicity, our period is $2 \pi$. We want to determine when and in what sense a Fourier series converges.

    First, we see that $\sum a_n e^{inx}$ converges if $\sum \abs{a_n} < \infty$, as in $a_n \in \ell_1$. Also, if $(a_n) \in \ell_2$, we get convergence.

    \begin{prop}
        $\{e_n(x)\}$ is a complete orthonormal basis on $C(\T)$.
    \end{prop}
    The idea of the proof is to set $t=e^{ix}$ and follow the Weierstrass approximation theorem. For details, see the text.
    
    \subsubsection{Distributions and Fourier Transform}
    We will begin discussing Fourier transforms, which are more general than Fourier series.
    
    \begin{rmk}
        Fourier transforms commute with convolution.
    \end{rmk}

    \begin{thm}[Carleson, 1966]
        If $f \in L^2$, then $\sum a_n e^{ink} \to f$ pointwise almost everywhere, However, convergence fails at points of discontinuity of $f$.\footnote{This is known as the Gibbs phenomenon.}
    \end{thm}

    Note that the power of Fourier series and the FFT is the speed with which this is calculated, which is $O(n \log n)$. Wavelets are also efficient, but they are discontinuous, so they avoid Gibbs phenomena and can be used in image processing.

    We know that $D(e^{inx}) = ine^{inx}$, so $D(\sum a_n e^{inx}) = \sum ina_n e^{inx}$. Differentiating the Fourier series just scales the modes, so differentiating $k$ times should lead to the Fourier series of $k$-th derivatives. Thus $f$ is $k$-times differentiable if and only if $\sum \abs{a_n}^2 n^{2k}$ converges. This is one way to define the Sobolev space $H^k(\T)$.

    \begin{defn}[Fourier Transform]
        Let $u \in L^1(\R^d)$. Then the Fourier transform is \[\widehat{u}(k) = \frac{1}{(2\pi)^{d/2}} \int_{\R^d} e^{-ik\cdot x} u(x) dx.\]

        The inverse transform is given by \[ \overset{\vee}{v}(x) = \frac{1}{(2\pi)^{d/2}} \int_{\R^d} e^{ik\cdot x} v(k) dk. \]
    \end{defn}

    We think of $x$ being physical space and $k$ being frequency space.

    \begin{thm}[Plancharel]
        If $u \in L^1(\R^d) \cap L^2(\R^d)$, then $\widehat{u} \in L^2(\R^d)$ and $\norm{\widehat{u}} = \norm{u}$ in $L^2$.

        \begin{proof}[Idea of Proof]
            By our assumptions, the transform exists everywhere. Then we compute
            \begin{align*}
                \norm{\widehat{u}}^2 &= \int_k \overline{\widehat{u}(k)} \widehat{u}(k) dk \\
                                     &= \frac{1}{(2\pi)^d} \int \int \int e^{iky} \overline{u}(y) e^{-ikx} u(x) dx \ dy\ dk \\
                                     &= \frac{1}{(2\pi)^d} \int \int \int e^{ik(y-x)} dk \overline{u}(y)u(x) dx \ dy.
            \end{align*}
            If $u, \widehat{u}$ have compact support, then we can integrate to get $\frac{1}{(2\pi)^{d/2}} \delta_{x,y}$. This all needs to be checked.

            Taking this for granted, we get that the norm is $\int \overline{u}(x) u(x) dx = \norm{u}^2$.
        \end{proof}
    \end{thm}

    It follows that we can extend the Fourier transform to be defined on all of $L^2$. To do this, we approximate $u \in L^2$ be $u_n \in L^2 \cap L^1$ and take limits.

    \begin{prop}
        \begin{enumerate}
            \item $\int u(x)\overline{v}(x) dx = \int \widehat{u}(k) \overline{\widehat{v}}(k) dk$.
            \item $\widehat{(u * v)} = (2\pi)^{d/2} \widehat{u} \widehat{v}$.
            \item $(\widehat{u})^{\vee} = u$.
            \item If $D^{\alpha} u \in L^2$, then $\widehat{D^{\alpha} u} = (ik)^{\alpha} \widehat{u}$ for $\alpha \in \Z^d$.\footnote{Here we are using multi-index notation with attached multinomial coefficients.}
        \end{enumerate}
    \end{prop}

    \subsection{Lecture 23 (Apr 18)}
    We will define the notion of a distribution, following the note from Smoller on the webpage.

    \begin{defn}[Distribution]
        A distribution is a continuous linear functional on $C^{\infty}_0$ the closure of $C^{\infty}_C$, which is the space of compactly supported smooth function.
    \end{defn}

    Here we have avoided the mention of a topology because $C^{\infty}_0$ is not metrizable. We define the strong topology:

    \begin{defn}[Strong Topology]
        We declare $\phi_j \to 0$ in the strong topology if $\norm{D^{\alpha}_{\phi_j}}_{\infty} \to 0$ for all nonzero nulti-indices.
    \end{defn}
       
    We call the $\phi \in C_0^{\infty}$ test dunctions. Then the disributions are continuous in the weak topology, where $T_j \to 0$ if $\gen{T_j,\phi} \to 0$ for all $\phi \in C_0^{\infty}$.

    \begin{exm}
        \begin{enumerate}
            \item If $f \in L^1_{loc}$, then $\gen{T_a,\phi} = T_f(\phi) = \int_{\R^d} f(x) \phi(x) dx$.
            \item Evaluation at $a$, given by $\gen{\delta_a,\phi} = \delta_a(\phi) = \phi(a)$.
            \item Any Radon measure $\mu$ gives $\gen{T_{\mu},f} = \int_{\R^d} \phi(x) d \mu$.
            \item Evaluation of derivatives: For $a \in \R^d, \alpha$ multiindex, $\gen{T_{a,\alpha},\phi} = (-1)^{\alpha} D^{\alpha}\phi(a)$.
        \end{enumerate}
    \end{exm}

    We define the support of a distribution $T$.
    \begin{defn}
        We say $T$ vanishes on a set $U \subset \R^d$ if $\gen{T,\phi} = 0$ for all $\phi$ whose support is a subset of $U$. The support $\operatorname{supp} T = \{x \in \Omega \mid \text{ there exists } U \ni x, T = 0 \text{ on } U \}$.
    \end{defn}

    \subsubsection{Weak Derivatives}
    These are defined by integration by parts. For example, if $u \in L^1_{loc}$ and $\alpha$ is a multi-index, $V$ is the $\alpha$-weak derivatives if integration by parts holds on all smooth test functions $\phi$. That is, $v$ is the $\alpha$-weak derivative of $u$ if for all test functions $\phi \in C_C^{\infty}$, $\int v \phi dx = (-1)^{|\alpha|} \int u D^{\alpha} \phi dx$.

    We can define this for any distribution. 
    \begin{defn}[Weak Derivative]
        Let $T$ be a distribution and $\alpha$ a multi-index. Then $D^{\alpha}T$ is defined by $\gen{D^{\alpha}T,\phi} = (-1)^{\abs{\alpha}} \gen{T,D^{\alpha} \phi}$.
    \end{defn}

    Note that we cannot multiply distributions, but we can multiply distributions by $C^{\infty}$ functions by $\gen{fT,\phi} = \gen{T,f\phi}$.

    \begin{exm}
        If $f$ is continuous at $0$, we can define $\gen{f \delta,\phi} = f(0) \phi(0)$. However, if $H$ is the Heaviside function, this does not make sense because $\delta = H'$, but $H=H^2$, so we don't know what to use.
    \end{exm}

    An application of this is Burger's equation: $u_t+uu_x=0$. This admits discontinuous solutions, so we can write it in the better form $u_t+\left(\frac{1}{2}u^2\right)_x=0$.\footnote{Thanks, physicists!}

    Recall that convolutions are like products. Then we can define \[\gen{f*g,\phi} = \int f*g(x) \phi(x) dx = \int f(y) \left( g(z) \phi(z+y) dz \right)dy.\] We can rewrite this as $Tf_y(Tg_z \phi(y+z))$. We can use this to define convolutions for distibutions.

    \subsection{Lecture 24 (Apr 23)}
    Recall the notion of a distribution. Today we will define the convolution of two distributions. 
    \begin{defn}[Convolution of Distributions]
        The convolution of two distributions $S,T$ is defined by $\gen{S*T,\phi} = \gen{S,\gen{T,\phi(y+z)}_z}_y$. Here, we require at least one of $S,T$ has compact support.
    \end{defn}

    \begin{lem}
        Suppose $T$ is a distribution and $S$ is a compactly supported distribution.
        \begin{enumerate}
            \item $T*S$ is a distribution.
            \item $T*S=S*T$
            \item If $U$ is a compactly supported distribution then $(T*S)*U = T*(S*U)$
            \item $\operatorname{supp}(T*S) \subset \operatorname{supp}T + \operatorname{supp}S$.
            \item If $f \in C_0^{\infty}$ and $T$ is a distribution, then regarding $f$ as a distribution, $f*T(x) = \gen{T,f(x-y)}_y$.
        \end{enumerate}
    \end{lem}

    \begin{rmk}
        Note that the delta function is the unit with respect to convolution.
    \end{rmk}

    \begin{lem}
        Convolution commutes with differentiation.
        \begin{proof}
            \begin{align*}
                \gen{D^{\alpha}(T*S),\phi} &= (-1)^{\abs{\alpha}} \gen{T*S,D^{\alpha} \phi} \\
                                           &= (-1)^{\abs{\alpha}} \gen{T,\gen{S,D^{\alpha} \phi(y+z)}}\\
                                           &= \gen{T,(-1^{\abs{\alpha}})\gen{S,D^{\alpha} \phi(y+z)}}\\
                                           &= \gen{T, \gen{D^{\alpha}S, \phi(y+z)}} \\
                                           &= \gen{T*D^{\alpha}S, \phi}
            \end{align*}
        \end{proof}
    \end{lem}

    We will use the same tricks to define the Fourier transform of some distributions. We have too many distributions, so we extend our class of test functions.

    \begin{defn}
        Schwartz space $\mc{S}$ is the set of those $\varphi \in C^{\infty}$ such that for all multi-indices $\alpha, \beta \in \Z_+^d$, \[\sup_{x \in \R^d} \abs*{x^{\alpha}D^{\beta} \varphi(x)} < \infty.\]
    \end{defn}

    Any $\varphi \in C^{\infty}$ such that $\abs{\varphi}\leq e^{-\ep \abs{x}^{\delta}}$ is in $\mc{S}$.

    Note the following properties of multi-indices.
    \begin{enumerate}
        \item Taylor: \[f(x) = \sum_{\abs{\alpha} \leq k} \frac{1}{\alpha !} D^{\alpha} f(x_0)(x-x_0)^{\alpha} + r_k(x), \] where $r_k(x) = o(\abs{x-x_0}^k)$ and $\alpha ! = \alpha_1! \cdots \alpha_d!$.
        \item Leibniz: \[D^{\alpha}(fg) = \sum_{\beta + \gamma = \alpha} \frac{\alpha !}{\beta!\gamma!} (D^{\beta}f)(D^{\gamma}g). \]
    \end{enumerate}

    We consider the topology of $\mc{S}$. Set $P_{\alpha, \beta}(\varphi) = \sup_x \abs{x^{\alpha}D^{\beta} \varphi(x)}$. This is not quite a norm but is a seminorm. Given a family of seminorms $P_{\alpha}$ on $\mc{Z}$, we define a topology on $\mc{Z}$ by $\mc{N}$ is an open neighborhood of $z$ if $\mc{N} = \{y \mid p_{\alpha}(y-z) < \ep \text{ for some finite set of }\alpha \}$. The $p_{\alpha}$ separate points if $p_{\alpha}(y) = 0$ implies $y = 0$ for all $\alpha$. If the family separates points, the topology is Hausdorff. In this case $\mc{Z}$ is locally convex.

    $\mc{Z}$ is also metrizable if $A$ is countable. We need to take account of all the seminorms: \[d(x,y) = \sum_n \frac{1}{2^n} \frac{p_n(x-y)}{1+p_n(x-y)} \]

    \begin{thm}
        In this metric, $\mc{S}$ is complete.
        \begin{proof}
            Consider a Cauchy sequence $\varphi_n$. Then $p_{0,0}(\varphi_n-\varphi_m) < \ep$. 
            Also by definition of the $p_{\alpha,\beta}$, the sequence is Cauchy in $L^{\infty}$. 
            Thus the limit $\widetilde{\varphi}$ is continuous. Then using $p_{0,\beta}$ we get a Cauchy sequence for the derivatives. We need to prove that the limits coincide with the derivarives of the limit $\widetilde{\varphi}$.

            We do this by induction on $\abs{\beta}$. Suppose $\abs{\beta} = k+1$. Then there exists $j$ such that $\beta = \alpha + e_j$ where $\abs{\alpha} = k$. Now write \[D^{\alpha} \varphi_n(x+te_j) - D^{\alpha} \varphi_n(x) = \int_0^t \frac{\partial}{\partial x_j} \left( D^{\alpha} \varphi_n(x+se_j) \right) ds = \int_0^t D^{\beta} \varphi_n(x+se_j) ds. \] Taking the limit as $n \to \infty$, we see that \[D^{\alpha} \widetilde{\varphi}(x+te_j) - D^{\alpha}\widetilde{\varphi(x)} = \int_0^t \psi_{\beta}(x+se_j)ds.\] Dividing by $t$ and taking $t \to 0$, we see that $\frac{\partial}{\partial x_j} D^{\alpha} \widetilde{\varphi(x)} = \psi_{\beta}(x)$ by continuity. This gives that $x^{\alpha}D^{\beta} \varphi_b$ is Cauchy and thus converges to $x^{\alpha}D^{\beta}\widetilde{\varphi}$.
        \end{proof}
    \end{thm}

    \subsection{Lecture 25 (Apr 25)}
    We continue our discussion of Schwarz space. We will apply the Fourier transform to $\mc{S}$ and see what happens. Recall that the Fourier transform morally decomposes a function into its pieces which look like $e^{ik\cdot x}$ with wavenumber $k$. We know $\mc{F}:L^1 \to L^{\infty}$ and $\norm{\widehat{f}(k)}\leq \norm{f} \sup \frac{\abs{e^{ikx}}}{(2\pi)^{d/2}}$. In fact, if $f \in L^1$, then the Fourier transform is continuous.

    Not we introduce the translation operator $\tau_h f(x) = f(x-h)$. Then a simple calculation shows that $\widehat{\tau_h f} = e^{-ikh} \widehat{f}$. Assuming $f$ is $C^1$, note that the Fourier transform of $\frac{1}{h} (\tau_{-h}f - f)$ is $\frac{e^{ikh}-1}{h} \widehat{f}$. Taking the limit as $h \to 0$, we conclude that $\widehat{f'(x)} = ik \widehat{f}$.

    Next we consider the Fourier transform of a convolution. This was on the homework last semester, so the calculation is omitted (just use Fubini): \[\widehat{f*g} = (2\pi)^{d/2} \widehat{g} \widehat{f}.\] 

    Define $\mc{G}g = \frac{1}{(2\pi)^{d/2}} \int_{\R^d} e^{ikx} g(k) dk$. We will calculate 
    \begin{align*}
        \mc{G}\widehat{f} &= \frac{1}{(2\pi)^{d/2}} \int e^{ikx} \frac{1}{2\pi}^{d/2}e^{-iky} f(y) dy\ dk\\
                          &= \frac{1}{(2\pi)^d} \int \int e^{ik(x-y)}f(y) dy\ dk \\
                          &= \frac{1}{(2\pi)^d} \int \left( \int e^{ik(x-y)} dk \right) f(y) dy
    \end{align*}

    We know that $\int e^{ik(x-y)} dx = c \delta(x-y)$. Then formally, $\widehat{\delta} = \frac{1}{(2\pi)^{d/2}}$. In order to justify this, we will need to consider tempered distributions. The tempered distributions are elements of $\mc{S'}$, the dual of $\mc{S}$. Note that $\widehat{D^{\alpha}f} = (ik)^{\alpha} \widehat{f}$. Thus $\mc{F}$ maps $\mc{S}$ to itself. Assuming $f,g$ are nice enough, we can see that $\gen{\widehat{f},g} = \gen{\widehat{g},f} = \gen{f,\widehat{g}}$. Therefore, given $T \in \mc{S'}$, we define $\widehat{T}$ by $\gen{\widehat{T}, \varphi} = \gen{T,\widehat{\varphi}}$.

    We conclude the calculation with 
    \begin{align*}
        \mc{G} \widehat{f} &= \lim \frac{1}{(2\pi)^d} \int_{-M}^M e^{ikx} \int e^{-iky} f(y) dy dk \\
                           &= \lim \frac{1}{(2\pi)^d} \int_{\R^d} \int_{-M}^M e^{ik(x-y)} dx f(y) dy \\
                           &= \lim \frac{1}{(2\pi)^d} \int \frac{1}{i(x-y)}e^{ik(x-y)} \Biggl\lvert_{iM}^M f(y) dy \\
                           &= \lim \frac{1}{(2\pi)^d} \int \frac{e^{iM(x-y)} - e^{-iM(x-y)}}{i(x-y)} f(y) dy \\
                           &= \lim \frac{1}{(2\pi)^d} \int 2M \frac{\sin M(x-y)}{M(x-y)} f(y) dy.
    \end{align*}
    The sine term is a kernal approaching $\delta$ as $M \to \infty$, which will be confirmed later.

    \subsubsection{Yet another differential equation}
    Consider a linear hyperbolic PDE $P(D)v = f$. Formally, we apply the Fourier transform. First we solve the easier problem $P(D)v = \delta$, where $P(D)$ is some element of the algebra of differential operators. We can apply the Fourier transform here, find the Fourier transform formally, and then invert. Because $\delta*f = f$, $v*f$ solves $P(D)v = f$.
    
    \subsection{Lecture 26 (Apr 30)}
    We go back to the inversion formula.

    \begin{thm}
        If $f,g \in L^{\infty} \cap L^2 \cap L^1$ then $\mc{G} = \mc{F}^{-1}$.
        \begin{proof}
            First restrict to $d=1$. By Fubini, $d > 1$ follows immediately. Then for $d=1$ we have
            \begin{align*}
                \mc{G}\widehat{f} &= \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} e^{ikx} \frac{1}{\sqrt{2\pi}}\int_{\R} e^{-iky} f(y) dy dk \\
                                  &= \lim_{M \to \infty} \frac{1}{2\pi} \int_{-M}^M e^{ikx} e^{-iky} f(y) dy dk \\
                                  &= \lim_{M \to \infty} \int_{\R} f(y) \left( \frac{1}{2\pi} \int_{-M}^M e^{ik(x-y)} dk \right) dy \\
                                  &= \lim_{M \to \infty} S_M * f
            \end{align*}
            where $S_m(x) = \frac{1}{2\pi} \int_{-M}^{M} e^{ikx} dk = \frac{1}{\pi} \frac{\sin Mx}{x}$. Therefore $S_M(x) = \frac{M}{\pi} \operatorname{sinc} Mx$. We know that $\int_{-\infty}^{\infty} \operatorname{sinc} x = \pi$ (proved using complex analysis). Thus $\int S_M(x) = 1$. Then for $M$ large, we have $S_m \to \delta$ as distributions. To justify this, we calculate:
            \begin{align*}
                \int_{-\infty}^{\infty} \frac{1}{\pi} \frac{\sin Mx}{x} \varphi(x) dx &= \int_{\abs{x} \geq 1} + \int_{\abs{x} \leq 1}.
            \end{align*}
            Integrate by parts to get 
            \begin{align*}
                \int_{1}^R \frac{1}{\pi} \frac{\sin Mx}{x} \varphi(x) dx &= \frac{1}{\pi} \left( \frac{-1}{M} \cos Mx \frac{\varphi(x)}{x} \right)\Bigl\lvert_{1}^R - \int_1^R \frac{-1}{M} \cos Mx \frac{1}{\pi} \left( \frac{\varphi}{x} \right)' dx \\
                                                                         &= O(1) \frac{1}{M} \to 0
            \end{align*}
            because $\varphi \in \mc{D}$. Now we see that
            \begin{align*}
                \int_{\abs{x} \leq 1} \frac{1}{\pi} \frac{\sin Mx}{x} \varphi(x) dx &= \frac{1}{\pi} \int_{\abs{x} \leq 1} \sin Mx \frac{\varphi(x) - \varphi(0)}{x} dx + \frac{\varphi(0)}{\pi} \int_{\abs{x} \leq 1} \frac{\sin Mx}{x} dx \\
                                                                                    &= \frac{1}{\pi} \int_{\abs{x} \leq 1} \sin (Mx) \psi(x) dx + \frac{\varphi(0)}{\pi} \int_{\abs{x} \leq 1} \frac{\sin Mx}{x} dx.
            \end{align*}
            The first integral yields $1/M$ again, and the second integral becomes $\frac{\varphi(0)}{\pi} \int_{\abs{y} \leq M} \operatorname{sinc}(y) dy \to 0 $ as $M \to \infty$.
        \end{proof}
    \end{thm}

    We consider more examples. For example, if $f \in L^1$, this determines a distribution. However, if $f \notin L^1$, can we still define a corresponding distribution? (As an example, take $1/x$). Generally, it is not really possible. We lose ``balance'' by using $\varphi$ weighted artificially to one side of the singularity at $0$. On the other hand, if $\varphi$ is even, we can use the ``principal value'' distribution:

    The principal value $p.v.1/x$ is defined by $\gen{p.v.1/x, \varphi} = \lim_{\ep \to 0} \int_{\abs{x} \geq \ep} \frac{\varphi(x)}{x} dx$. We show that this is a distribution. Write 
    \begin{align*}
        \int_{\abs{x} \geq \ep} \frac{\varphi(x)}{x} dx = \int_{\ep}^{\infty} \frac{\varphi(x) - \varphi(-x)}{x} dx \to \int_0^{\infty} \frac{\varphi(x) - \varphi(-x)}{x} dx.
    \end{align*}
    Now observe that $\abs*{\frac{\varphi(x) - \varphi(-x)}{x}} = \abs*{\frac{1}{x} \int_{-x}^x \varphi'(t) dt} \leq 2 \norm{\varphi'}_{\infty}$, wo the integrand is bounded. Now write 
    \begin{align*}
        \abs{\gen{p.v.1/x,\varphi}} &= \abs*{\int_0^{\infty}\frac{\varphi(x) - \varphi(-x)}{x} dx} \\
                                    &\leq \int_0^1 \abs*{\frac{\varphi(x) - \varphi(-x)}{x} dx} + \int_1^{\infty} \frac{\abs{x(\varphi(x) - \varphi(-x))}}{x^2} dx \\
                                    &\leq 2 \norm{\varphi'} + 2 \norm{x \varphi(x)}\int_1^{\infty} \frac{1}{x^2} dx
    \end{align*}

    Now consider the fimal example, which is the PDE $P(D)u = f$ from last time. Given a polynomial $P(D)$, its symbol is the polynomial $p(ik)$. Observe that $\widehat{Df} = \int e^{-ikx} Df \d x = -\int D(e^{-ikx}) f \d x = ik\widehat{f}$. Therefore a polynomial operator corresponds to a polynomial in Fourier space, and we can do this for more general functions. If $a$ is a function of $(ik)$, we can define an operator in physical space corresponding to $a$, which extends polynomials to power series and allows so-called ``Fractional Laplacians.''

    For example, in $\R^1$, the Laplacian has $\widehat{\Delta u} = (ik)^u \widehat{u} = -k^2\widehat{u}$. Thus we can define the operator $\sqrt{-\Delta}$ by $\sqrt{-\Delta} u = \overset{\vee}{\abs{k}\widehat{u}}$. This is a radially symmetric derivative. Similarly, the operator $\land$ defined by $\widehat{\land} = (1+k^2)^{1/2}$ is very useful for defining Sobolev spaces. The fractional Sobolev space can be defined by $H^s = \{ u \mid \land^s u \in L^2 \}$.
    

\end{document}
